{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYfZGYIAy0aT"
   },
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARfxxCIwzh0w"
   },
   "source": [
    "## Contents\n",
    "\n",
    "* LLM Creation\n",
    "* Creating Messages (System, Huaman, AI)\n",
    "* Creating Prompt Templates (SystemMessage, HumanMessage, AIMessage, Chat, PromptTemplate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDEgLyH72crR"
   },
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06153cc0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "llm_key: str = userdata.get('NVIDIA_API_KEY')\n",
    "os.environ[\"NVIDIA_API_KEY\"] = llm_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQWPy1ibHe_u"
   },
   "source": [
    "---\n",
    "If you prefer to load from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1764990925409,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "ikVBWCVnHlpT",
    "outputId": "9168da15-89c4-415b-c39c-3650ad24bd03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0ucDnewIEc8"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7927f54"
   },
   "source": [
    "First, we need to install the `langchain-nvidia-ai-endpoints` package to interact with NVIDIA's AI models via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6868,
     "status": "ok",
     "timestamp": 1765013043657,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "b24d2e67",
    "outputId": "7765909c-14a5-4853-fd61-d20f468f8620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aa617f3"
   },
   "source": [
    "Now, let's import `ChatNVIDIA` and initialize our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4e5b9fdb"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model='meta/llama-3.1-405b-instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGHWO23RIMyB"
   },
   "source": [
    "---\n",
    "OpenAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ2HAZDxIaTd"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsqSN9CcIPgM"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mSQ8iAeJIf0"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b462d54e"
   },
   "source": [
    "You can now use this `llm` object to interact with the model. Here's a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1255,
     "status": "ok",
     "timestamp": 1764951981958,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "f61cb23c",
    "outputId": "d796c03a-a9c3-4af9-9e68-c752afff228d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Who are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6AmZIEBEZJg"
   },
   "source": [
    "To view the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1764042944387,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "t9_S_g4TJqCr",
    "outputId": "cb7fe012-62ba-409c-b8bf-3cbaf7d24547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"',\n",
       " 'token_usage': {'prompt_tokens': 15,\n",
       "  'total_tokens': 37,\n",
       "  'completion_tokens': 22},\n",
       " 'finish_reason': 'stop',\n",
       " 'model_name': 'meta/llama-3.1-405b-instruct'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E2KTzadJ6Au"
   },
   "source": [
    "## MESSAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TO7dQxvzKSYV"
   },
   "source": [
    "Create a conversation with SystemMessage, AIMessage and UserMessage\n",
    "\n",
    "* SystemMessage: This object is responsible to set persona of the LLM / pass instructions to LLM.\n",
    "* HumanMessage: This object is responsible to help user ask query to the LLM\n",
    "* AIMessage: Response generated by the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aRee58JJ7bs"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7500,
     "status": "ok",
     "timestamp": 1764056950830,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "yN3ndCDEKwFe",
    "outputId": "1a2efb49-fac0-41b6-e047-66305367591b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a seasoned Mobile App Developer with over 10 years of experience in designing, developing, and deploying innovative mobile apps for top tech companies like Google, Apple, and Meta. My expertise spans across multiple platforms, including iOS, Android, and cross-platform frameworks like React Native and Flutter.\n",
      "\n",
      "I've had the privilege of working on various high-profile projects, from social media and entertainment apps to productivity and utility apps. My passion lies in crafting seamless user experiences, optimizing app performance, and staying up-to-date with the latest trends and technologies in the mobile app development space.\n",
      "\n",
      "Throughout my career, I've collaborated with cross-functional teams, including designers, product managers, and QA engineers, to deliver high-quality apps that meet and exceed user expectations. I'm well-versed in Agile development methodologies, version control systems like Git, and cloud-based services like Firebase and AWS.\n",
      "\n",
      "What brings you here today? Are you looking for advice on a mobile app project, or perhaps seeking insights on the latest mobile app development trends? I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a professional Mobile App Developer with 10+ years of experience in working for companies like Google, Apple and Meta.\"),\n",
    "    AIMessage(content=\"Hi, im Mobi. How can i assist you?\"),\n",
    "    HumanMessage(content=\"Who are you?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9LCxQsT_lKA"
   },
   "source": [
    "**To Stream the output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6608,
     "status": "ok",
     "timestamp": 1764057036111,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "2Cpl1VgJ--Ln",
    "outputId": "7da85c2a-db5e-406b-d7d8-b100e8e75af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a seasoned Mobile App Developer with over 10 years of experience in designing, developing, and deploying scalable, secure, and user-friendly mobile applications for various platforms, including iOS and Android.\n",
      "\n",
      "Throughout my career, I have had the privilege of working with some of the biggest tech giants in the industry, including Google, Apple, and Meta (formerly Facebook). My expertise spans multiple programming languages, frameworks, and technologies, such as Java, Swift, Kotlin, React Native, Flutter, and more.\n",
      "\n",
      "I have a strong passion for creating mobile experiences that delight users and exceed client expectations. My expertise includes:\n",
      "\n",
      "* Mobile app architecture and design\n",
      "* iOS and Android app development\n",
      "* Cross-platform app development (React Native, Flutter, etc.)\n",
      "* Mobile app security and compliance\n",
      "* Cloud-based services integration (AWS, Firebase, etc.)\n",
      "* Agile development methodologies\n",
      "* Team leadership and collaboration\n",
      "\n",
      "I'm always excited to share my knowledge, experience, and insights with others and help shape the future of mobile app development. How can I assist you today?"
     ]
    }
   ],
   "source": [
    "for token in llm.stream(messages):\n",
    "  print(token.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRALJBC0N2h8"
   },
   "source": [
    "## PROMPT TEMPLATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9n5n4uwN-Eg"
   },
   "source": [
    "Prompt Templates: Structured, reusable text templates used in LLM apps to standardize and format instructions or queries you send to an AI model | LLM model.\n",
    "\n",
    "\n",
    "**Types of Prompt Templates**\n",
    "\n",
    "* SystemMessagePromptTemplate: This template is used for generating system messages that provide model context or persona.\n",
    "* HumanMessagePromptTemplate: This template is used for geenrating human message (representing user input)\n",
    "* AIMessagePromptTemplate: Template for generating AI message, representing response from the assistant\n",
    "* PromptTemplate: Basic Template class for creating prompts with static text and variable placeholders\n",
    "* ChatPromptTemplate: Template for creating prompts with a sequence of message types in chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6F90lB1N2RU"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,           # Simple string templates\n",
    "    ChatPromptTemplate,       # Chat-style prompts with messages\n",
    "    SystemMessagePromptTemplate,  # For system messages\n",
    "    HumanMessagePromptTemplate,   # For user messages\n",
    "    AIMessagePromptTemplate,      # For assistant messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1169,
     "status": "ok",
     "timestamp": 1764054676812,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "m49xIQEJVVFb",
    "outputId": "dff299fb-d7e0-4c25-aa4b-2ea7e31a165f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Tell me a joke about cats\n",
      "<class 'str'>\n",
      "------\n",
      "Why did the cat join a band?\n",
      "\n",
      "Because it wanted to be the purr-cussionist! (get it?)\n"
     ]
    }
   ],
   "source": [
    "# Simple template\n",
    "simplePT = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "formattedString = simplePT.format(topic=\"cats\")\n",
    "\n",
    "print('------')\n",
    "print(formattedString)\n",
    "print(type(formattedString))\n",
    "print('------')\n",
    "\n",
    "response = llm.invoke(formattedString)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XErkvOU_ALMH"
   },
   "source": [
    "---\n",
    "**Alternate aproach**\n",
    "\n",
    "This works only for `PromptTemplate` and not for any other type of message prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqnsCffQ86Lm"
   },
   "outputs": [],
   "source": [
    "simplePT = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip2Vr6VhBVpy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1764056403171,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "lT2MwEMhWoTs",
    "outputId": "c1517bcd-1202-4b4e-bdda-837f8fb4e9d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "prompt=PromptTemplate(input_variables=['persona'], input_types={}, partial_variables={}, template='Act as a {persona} mobile app developer. You answer in short sentences') additional_kwargs={}\n",
      "<class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'>\n",
      "------\n",
      "------\n",
      "prompt=PromptTemplate(input_variables=['points', 'topic'], input_types={}, partial_variables={}, template='Tell me about the {topic} in {points} points') additional_kwargs={}\n",
      "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "------\n",
      "------\n",
      "content='Act as a Flutter mobile app developer. You answer in short sentences' additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "------\n",
      "content='Tell me about the State Management in 3 points' additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "systemPT = SystemMessagePromptTemplate.from_template('Act as a {persona} mobile app developer. You answer in short sentences')\n",
    "print('------')\n",
    "print(systemPT)\n",
    "print(type(systemPT))\n",
    "print('------')\n",
    "\n",
    "humanPT = HumanMessagePromptTemplate.from_template('Tell me about the {topic} in {points} points')\n",
    "print('------')\n",
    "print(humanPT)\n",
    "print(type(humanPT))\n",
    "print('------')\n",
    "\n",
    "systemMessage = systemPT.format(persona='Flutter')\n",
    "humanMessage = humanPT.format(topic='State Management', points=3)\n",
    "\n",
    "print('------')\n",
    "print(systemMessage)\n",
    "print(type(systemMessage))\n",
    "print('------')\n",
    "print(humanMessage)\n",
    "print(type(humanMessage))\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1764048059198,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "uOMtPfXUYbD3",
    "outputId": "58da2032-7cf6-4244-b798-8146f9d249c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input_variables=[] input_types={} partial_variables={} messages=[SystemMessage(content='Act as a Flutter mobile app developer. You answer in short sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the State Management in 3 points', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Create Chat prompt template using already created messages - Method 1\n",
    "\n",
    "chatPT = ChatPromptTemplate.from_messages([systemMessage, humanMessage])\n",
    "print('------')\n",
    "print(chatPT)\n",
    "print(type(chatPT))\n",
    "print('------')\n",
    "\n",
    "# To Create Chat Message\n",
    "chatMessages = chatPT.invoke(input={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3fzp038EH8b"
   },
   "source": [
    "---\n",
    "**Alternate approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX-buocHEQES"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
    "Do not respond with more than one word.\n",
    "\n",
    "Review: {review}\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "chatPT = ChatPromptTemplate.from_template(prompt)\n",
    "print('------')\n",
    "print(chatPT)\n",
    "print(type(chatPT))\n",
    "print('------')\n",
    "\n",
    "chatMessages = chatPT.invoke(\n",
    "    input={\n",
    "      'review': 'Awesome product.'\n",
    "    }\n",
    ")\n",
    "\n",
    "print('------')\n",
    "print(chatMessages)\n",
    "print(type(chatMessages))\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-bDGpfvEO1Z"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1764048111487,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "QF2VctEjZZ97",
    "outputId": "e8d22bb2-664d-4fca-b2e1-fb1e709ed777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input_variables=[] input_types={} partial_variables={} messages=[SystemMessage(content='Act as a Flutter mobile app developer. You answer in short sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the State Management in 3 points', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Create Chat prompt template using already created messages - Method 2\n",
    "\n",
    "chatPT = ChatPromptTemplate([systemMessage, humanMessage])\n",
    "print('------')\n",
    "print(chatPT)\n",
    "print(type(chatPT))\n",
    "print('------')\n",
    "\n",
    "# To Create Chat Message\n",
    "chatMessages = chatPT.invoke(input={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1764048117220,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "feOwxo4_ZmZ3",
    "outputId": "854f37fa-2da8-40fd-bcca-c463a5818f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input_variables=['persona', 'points', 'topic'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['persona'], input_types={}, partial_variables={}, template='Act as a {persona} mobile app developer. You answer in short sentences'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['points', 'topic'], input_types={}, partial_variables={}, template='Tell me about the {topic} in {points} points'), additional_kwargs={})]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Create Chat prompt template using directly Prompt Templates\n",
    "\n",
    "chatPT = ChatPromptTemplate([systemPT, humanPT])\n",
    "print('------')\n",
    "print(chatPT)\n",
    "print(type(chatPT))\n",
    "print('------')\n",
    "\n",
    "\n",
    "# To Create Chat messages when imput paramenters are required\n",
    "chatMessages = chatPT.invoke(input={\n",
    "    'persona': 'Flutter',\n",
    "    'topic': 'State Management',\n",
    "    'points': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1764048118940,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "cIUEcAEyZmNe",
    "outputId": "a2f05688-154b-42b7-cba4-add72a04caf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "messages=[SystemMessage(content='Act as a Flutter mobile app developer. You answer in short sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the State Management in 3 points', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "print('------')\n",
    "print(chatMessages)\n",
    "print(type(chatMessages))\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3355,
     "status": "ok",
     "timestamp": 1764048137593,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "p0zrrTb_alHP",
    "outputId": "df90afdb-8ef4-445a-ef53-69362e85d35f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 key points about State Management in Flutter:\n",
      "\n",
      "1. **Stateful vs Stateless**: Stateful widgets can change and redraw, while stateless widgets remain the same once built.\n",
      "2. **Provider and Consumer**: Provider library helps manage state by wrapping the app in a provider widget and using Consumer to access state in child widgets.\n",
      "3. **Bloc Pattern**: Business Logic Component (BLoC) separates presentation and business logic, making it easier to manage complex app states.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(chatMessages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpu83xr8y5tB"
   },
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_AYCyUdzhQd"
   },
   "source": [
    "### Contents\n",
    "\n",
    "* LCEL Basics\n",
    "* Sequential Chain Invoke\n",
    "* Parallel Chain\n",
    "* Composed Sequencial - Passing Data to next Runnable\n",
    "* Router Chain\n",
    "* Custom Chain Runnables (RunnableLambda)\n",
    "* Custom Chain Creation (@chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYfmBE43g4P1"
   },
   "source": [
    "## Langchain Expression Language (LCEL) Basics\n",
    "\n",
    "-  Using LangChain Expression Language any two runnables can be \"chained\" together into sequences.\n",
    "- The output of the previous runnable's .invoke() call is passed as input to the next runnable.\n",
    "- This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing.\n",
    "\n",
    "\n",
    "- Type of LCEL Chains\n",
    "    - Sequential Chain\n",
    "    - Parallel Chain\n",
    "    - Router Chain\n",
    "    - Chain Runnables\n",
    "    - Custom Chain (Runnable Sequence)\n",
    "\n",
    "\n",
    "\n",
    "- Chaining Runnables: LCEL allows you to combine different components (called 'Runnables', such as LLMs, prompts, parsers, or custom functions) into a sequence. This means you can create complex logic by connecting simpler, modular pieces.\n",
    "- Input/Output Flow: The core idea is that the output of one runnable automatically becomes the input for the next runnable in the chain. This creates a clear, sequential flow of data and processing.\n",
    "- Pipe Operator (|): This is the most common and intuitive way to chain runnables. It visually represents the flow from left to right. The .pipe() method does the same thing but can be useful in more complex programmatic constructions.\n",
    "- Types of LCEL Chains: The types you listed are good categories to think about. For instance, a `Sequential Chain` is the basic left-to-right flow. `Parallel Chain` allows multiple runnables to execute simultaneously on the same input, and their outputs are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCTZfk7WjGJR"
   },
   "source": [
    "1. What is LCEL?\n",
    "\n",
    "> LCEL stands for LangChain Expression Language.\n",
    "\n",
    "> It provides a concise, composable, and Pythonic way to build chains, pipelines, and workflows in LangChain.\n",
    "\n",
    "> Goal: Replace the older SequentialChain, RouterChain, etc., with a more flexible and declarative API.\n",
    "\n",
    "> Key Concepts: Chains as functions (or callables), easy composition (| operator), clear input/output typing.\n",
    "\n",
    "2. Core LCEL Components\n",
    "\n",
    "> a. Runnable\n",
    "\n",
    ">> The core abstraction in LCEL.\n",
    "\n",
    ">> Any object that implements the .invoke() method and can be composed using the | operator.\n",
    "\n",
    ">> Examples: Models, PromptTemplates, Chains.\n",
    "\n",
    "> b. PromptTemplate\n",
    "\n",
    ">> Used to format inputs into prompts for LLMs or ChatModels.\n",
    "\n",
    ">> LCEL expects prompt templates to have variable names matching the input keys.\n",
    "\n",
    "> c. LLM and ChatModel\n",
    "\n",
    ">> Language Models and ChatModels from langchain_openai, langchain_community, etc.\n",
    "\n",
    ">> Compatible with LCEL if they implement .invoke()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DQg07Adksbv"
   },
   "source": [
    "### Standard Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2566,
     "status": "ok",
     "timestamp": 1764050877326,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "BBvcmMIzinnX",
    "outputId": "495ce35d-60a7-48d2-a647-8aaf8010d264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input_variables=['input_language', 'output_language', 'text'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]\n",
      "-----\n",
      "-----\n",
      "messages=[SystemMessage(content='You are a helpful assistant that translates English to Kannada.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n",
      "-----\n",
      "-----\n",
      "content='ನಾನು ಪ್ರೋಗ್ರಾಮಿಂಗ್ ಅನ್ನು ಪ್ರೀತಿಸುತ್ತೇನೆ.' additional_kwargs={} response_metadata={'role': 'assistant', 'content': 'ನಾನು ಪ್ರೋಗ್ರಾಮಿಂಗ್ ಅನ್ನು ಪ್ರೀತಿಸುತ್ತೇನೆ.', 'token_usage': {'prompt_tokens': 32, 'total_tokens': 105, 'completion_tokens': 73}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-405b-instruct'} id='lc_run--7fc7451b-8b9f-46b1-99a2-ec18e0fbe58a-0' usage_metadata={'input_tokens': 32, 'output_tokens': 73, 'total_tokens': 105} role='assistant'\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that translates {input_language} to {output_language}.\")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([system_template, human_template])\n",
    "\n",
    "print('-----')\n",
    "print(chat_prompt_template)\n",
    "print('-----')\n",
    "\n",
    "chat = chat_prompt_template.invoke({\"input_language\": \"English\", \"output_language\": \"Kannada\", \"text\": \"I love programming.\"})\n",
    "\n",
    "print('-----')\n",
    "print(chat)\n",
    "print('-----')\n",
    "\n",
    "response = llm.invoke(chat)\n",
    "\n",
    "print('-----')\n",
    "print(response)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBxCYWlLoPa9"
   },
   "source": [
    "### Chain Method - Sequencial - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6643,
     "status": "ok",
     "timestamp": 1764051795253,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "6nDkBfTdoRMz",
    "outputId": "fa724acb-74e3-4455-e6d2-30bb22eaff0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input_variables=['input_language', 'output_language', 'text'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]\n",
      "-----\n",
      "-----\n",
      "first=ChatPromptTemplate(input_variables=['input_language', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]) middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "-----\n",
      "-----\n",
      "ನಾನು ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆಯನ್ನು ಪ್ರೀತಿಸುತ್ತೇನೆ (Nānu kr̥taka buddhimatteyannu prītisuttēne)\n",
      "\n",
      "Note: Here's a more natural way to express the same sentence in Kannada: ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆಯ ಅಭಿಮಾನಿ ನಾನು (Kr̥taka buddhimatteya abhimāni nānu)\n",
      "<class 'str'>\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that translates {input_language} to {output_language}.\")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([system_template, human_template])\n",
    "\n",
    "print('-----')\n",
    "print(chat_prompt_template)\n",
    "print('-----')\n",
    "\n",
    "# Simple Chain\n",
    "# A chain / Runnable Sequence comprising of three Runnables:\n",
    "# 1. chat_prompt_template - containing templates\n",
    "# 2. LLM itself\n",
    "# 3. String output parser: It parses the model’s response and returns it as a plain Python string, removing any unnecessary metadata\n",
    "\n",
    "myChain = chat_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "print('-----')\n",
    "print(myChain)\n",
    "print(type(myChain))\n",
    "print('-----')\n",
    "\n",
    "response = myChain.invoke(input={\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"Kannada\",\n",
    "    \"text\": \"I love artificail intelligence\"\n",
    "})\n",
    "\n",
    "print('-----')\n",
    "print(response)\n",
    "print(type(response))\n",
    "print('-----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiwQeZGUsjFH"
   },
   "source": [
    "### Chain Method - Sequencial - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12193,
     "status": "ok",
     "timestamp": 1764052861249,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "K-A40qoCskSE",
    "outputId": "7f6fa971-7f09-41ae-ed38-2712bf182feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input_variables=['lines', 'output_language', 'text'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_language'], input_types={}, partial_variables={}, template='You are a top class poet in {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['lines', 'text'], input_types={}, partial_variables={}, template='Give me a {lines} lines of poem about the {text}'), additional_kwargs={})]\n",
      "-----\n",
      "-----\n",
      "first=ChatPromptTemplate(input_variables=['lines', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_language'], input_types={}, partial_variables={}, template='You are a top class poet in {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['lines', 'text'], input_types={}, partial_variables={}, template='Give me a {lines} lines of poem about the {text}'), additional_kwargs={})]) middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "-----\n",
      "-----\n",
      "ಕಲಾತ್ಮಕ ಬುದ್ಧಿಯ ಸೆಳೆತಕ್ಕೆ ನಾನು ಕೈ ಬಿಡೆ\n",
      "ಯಂತ್ರಗಳ ಹೃದಯದಲ್ಲಿ ಕಲೆಯಿಂದ ಸಾಕಾರ\n",
      "ಮಾನವತೆಯ ಕೃತಿ ಇದು, ನಾನು ಇಷ್ಟಪಡುತ್ತೇನೆ\n",
      "ಅವನ ಲೆಕ್ಕಗಳಲ್ಲಿ ಕಲೆಯಿಂದ ಒಂದು ಸಂಸಾರ\n",
      "ಕೃತಕ ಬುದ್ಧಿಯಲ್ಲಿ ನಾನು ಪ್ರೀತಿಗೆ ಸಿಲುಕುತ್ತೇನೆ\n",
      "\n",
      "(I love the allure of artistic intelligence\n",
      "In the heart of machines, a work of art takes shape\n",
      "This is a human creation, I love it\n",
      "In its calculations, a world of art is revealed\n",
      "I'm smitten with love for artificial intelligence)\n",
      "<class 'str'>\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_template_2 = SystemMessagePromptTemplate.from_template(\"You are a top class poet in {output_language}.\")\n",
    "human_template_2 = HumanMessagePromptTemplate.from_template(\"Give me a {lines} lines of poem about the {text}\")\n",
    "\n",
    "chat_prompt_template_2 = ChatPromptTemplate([system_template_2, human_template_2])\n",
    "\n",
    "print('-----')\n",
    "print(chat_prompt_template_2)\n",
    "print('-----')\n",
    "\n",
    "myChain_2 = chat_prompt_template_2 | llm | StrOutputParser()\n",
    "\n",
    "print('-----')\n",
    "print(myChain_2)\n",
    "print(type(myChain_2))\n",
    "print('-----')\n",
    "\n",
    "response = myChain_2.invoke(input={\n",
    "    \"output_language\": \"Kannada\",\n",
    "    \"lines\": 5,\n",
    "    \"text\": \"I love artificail intelligence\"\n",
    "})\n",
    "\n",
    "print('-----')\n",
    "print(response)\n",
    "print(type(response))\n",
    "print('-----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKuK4DP5uxcS"
   },
   "source": [
    "### Chain Method - Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A57xqi2qB-1O"
   },
   "source": [
    "**Parallel LCEL Chain**\n",
    "- Parallel chains are used to run multiple runnables in parallel.\n",
    "- The final return value is a dict with the results of each value under its appropriate key.\n",
    "\n",
    "\n",
    "**What is a Parallel Chain?**\n",
    "\n",
    "- Parallel chaining lets you run multiple chains (pipelines) simultaneously using the same or similar inputs, collecting all outputs together.\n",
    "\n",
    "- In LangChain LCEL, this is achieved via RunnableParallel.\n",
    "\n",
    "- It's analogous to a “fan-out” pattern, where you branch out your input to multiple tasks and gather all their results in one go.\n",
    "\n",
    "**Why Use Parallel Chains?**\n",
    "\n",
    "- To answer multiple questions about the same input in one shot.\n",
    "- To generate diverse content (e.g., facts, poems, summaries) from the same base information.\n",
    "- To save code and avoid running chains sequentially when there's no dependency between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8713,
     "status": "ok",
     "timestamp": 1764052935692,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "xs9Zuc98uziV",
    "outputId": "9cabe331-8812-4564-d052-9c6930ff440f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "steps__={'translate': ChatPromptTemplate(input_variables=['input_language', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])\n",
      "| ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})\n",
      "| StrOutputParser(), 'poem': ChatPromptTemplate(input_variables=['lines', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_language'], input_types={}, partial_variables={}, template='You are a top class poet in {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['lines', 'text'], input_types={}, partial_variables={}, template='Give me a {lines} lines of poem about the {text}'), additional_kwargs={})])\n",
      "| ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})\n",
      "| StrOutputParser()}\n",
      "<class 'langchain_core.runnables.base.RunnableParallel'>\n",
      "-----\n",
      "-----\n",
      "{'translate': 'ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)', 'poem': 'Here is a 5-line poem in Kannada about Artificial Intelligence:\\n\\n\\nಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\\nಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\\nಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\\nಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಡುಕುತಾನೆ\\nಸೃಷ್ಟಿಯ ರಹಸ್ಯವನ್ನು ಅರಿಯುತಾನೆ'}\n",
      "<class 'dict'>\n",
      "-----\n",
      "ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)\n",
      "-----\n",
      "Here is a 5-line poem in Kannada about Artificial Intelligence:\n",
      "\n",
      "\n",
      "ಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\n",
      "ಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\n",
      "ಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\n",
      "ಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಡುಕುತಾನೆ\n",
      "ಸೃಷ್ಟಿಯ ರಹಸ್ಯವನ್ನು ಅರಿಯುತಾನೆ\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "myParallelChain = RunnableParallel(\n",
    "    translate=myChain,\n",
    "    poem=myChain_2\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(myParallelChain)\n",
    "print(type(myParallelChain))\n",
    "print('-----')\n",
    "\n",
    "response = myParallelChain.invoke(input={\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"Kannada\",\n",
    "    \"text\": \"Artificial Intelligence\",\n",
    "    \"lines\": 5,\n",
    "})\n",
    "\n",
    "print('-----')\n",
    "print(response)\n",
    "print(type(response))\n",
    "print('-----')\n",
    "print(response['translate'])\n",
    "print('-----')\n",
    "print(response['poem'])\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFFbmO1fwtqc"
   },
   "source": [
    "### Chain Method - Composed Sequencial\n",
    "\n",
    "Passing data to next chain / runnable - using input parameter for next chain's prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9I4YcVfrzOhQ"
   },
   "source": [
    "**Straight forward method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9432,
     "status": "ok",
     "timestamp": 1764054083271,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "GTlbXbwiw5JD",
    "outputId": "c97e07ab-ace7-42f6-c23e-bca38eeef27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "The poem is written in Kannada, a Dravidian language spoken in the Indian state of Karnataka.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "analysis_template = PromptTemplate.from_template('''\n",
    "Analyse the following: {response}.\n",
    "You need to tell me which language it is written in.\n",
    "You just need to answer in one sentence.\n",
    "''')\n",
    "\n",
    "response_of_chain_2 = myChain_2.invoke({\n",
    "    \"output_language\": \"Kannada\",\n",
    "    \"lines\": 5,\n",
    "    \"text\": \"Artificial Intelligence\"\n",
    "})\n",
    "\n",
    "analysis_chain = analysis_template | llm | StrOutputParser()\n",
    "\n",
    "response_analysis_chain = analysis_chain.invoke({\n",
    "    \"response\": response_of_chain_2\n",
    "})\n",
    "\n",
    "print('-----')\n",
    "print(response_analysis_chain)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwpWSxjbzZR3"
   },
   "source": [
    "**Using composed chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10662,
     "status": "ok",
     "timestamp": 1764054280502,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "wF5X9WrDzg6l",
    "outputId": "efbbd792-52c6-4faa-f35b-fae923be8207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "The poem is written in Kannada, a Dravidian language spoken in the Indian state of Karnataka.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "analysis_template = PromptTemplate.from_template('''\n",
    "Analyse the following: {response}.\n",
    "You need to tell me which language it is written in.\n",
    "You just need to answer in one sentence.\n",
    "''')\n",
    "\n",
    "analysis_chain = analysis_template | llm | StrOutputParser()\n",
    "\n",
    "# 2nd Runnable is expecting \"response\" in it's input variable\n",
    "\n",
    "composed_chain = {\"response\" : myChain_2} | analysis_chain\n",
    "\n",
    "response_composed_chain = composed_chain.invoke({\n",
    "    \"output_language\": \"Kannada\",\n",
    "    \"lines\": 5,\n",
    "    \"text\": \"Artificial Intelligence\"\n",
    "})\n",
    "\n",
    "print('-----')\n",
    "print(response_composed_chain)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_RfghaQDZ-r"
   },
   "source": [
    "### Chain Router\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ch6bitiDlrO"
   },
   "source": [
    "- The router chain is used to route the output of a previous runnable to the next runnable based on the output of the previous runnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsJR7M8XG50-"
   },
   "source": [
    "**Sentiment analyzer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1764058917647,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "HFjx6offDtJM",
    "outputId": "e7e1bc96-979e-412d-a71c-2cc257ffdbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input_variables=['review'] input_types={} partial_variables={} template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n'\n",
      "-----\n",
      "-----\n",
      "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
      "-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
    "Do not respond with more than one word.\n",
    "\n",
    "Review: {review}\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template=prompt,\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(prompt_template)\n",
    "print('-----')\n",
    "\n",
    "sentiment_chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "print('-----')\n",
    "print(sentiment_chain)\n",
    "print('-----')\n",
    "\n",
    "review = \"The battery life is exceptional. I charged this device four weeks ago and have read two full novels and multiple articles without the battery indicator dropping below 60%. The screen also causes zero eye strain, even after hours of continuous reading. This device is the definitive upgrade.\"\n",
    "# review = \"The experience was severely disappointing. The wait time for the main courses was over an hour, and when the food finally arrived, the seafood platter was cold and clearly overcooked, tasting rubbery. The waiter was inattentive and provided no updates or apologies for the significant delay. The high prices are not justified by the subpar quality of the food and service. Avoid this place.\"\n",
    "\n",
    "sentiment_chain.invoke({\"review\": review})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McFKKoMqG-WJ"
   },
   "source": [
    "**Positive mail composer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5006,
     "status": "ok",
     "timestamp": 1764059633880,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "mXFGVLPFHHYh",
    "outputId": "a19bc2c6-27fa-4201-bdbc-f7bd9adda8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input_variables=['review'] input_types={} partial_variables={} template='\\nYou are expert in writing reply for positive reviews.\\nYou need to encourage the user to share their experience on social media.\\nReview: {review}\\nAnswer:\\n'\n",
      "-----\n",
      "-----\n",
      "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nYou are expert in writing reply for positive reviews.\\nYou need to encourage the user to share their experience on social media.\\nReview: {review}\\nAnswer:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
      "-----\n",
      "Wow, thank you so much for sharing your amazing experience with our device! We're thrilled to hear that the battery life has exceeded your expectations, and that you've been able to enjoy your favorite novels and articles without any interruptions. It's also great to know that our screen technology has been easy on your eyes, even during extended reading sessions.\n",
      "\n",
      "We'd love for you to share your experience with others! Would you consider posting about your experience on social media? A quick share on Facebook, Twitter, or Instagram can help others discover the benefits of our device. Your endorsement can make a huge difference in helping others find their perfect reading companion. Thank you again for your wonderful review, and we're so glad you're enjoying your device!\n"
     ]
    }
   ],
   "source": [
    "positive_mail_prompt = \"\"\"\n",
    "You are expert in writing reply for positive reviews.\n",
    "You need to encourage the user to share their experience on social media.\n",
    "Review: {review}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "positive_mail_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template=positive_mail_prompt,\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(positive_mail_prompt_template)\n",
    "print('-----')\n",
    "\n",
    "positive_mail_chain = positive_mail_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "print('-----')\n",
    "print(positive_mail_chain)\n",
    "print('-----')\n",
    "\n",
    "review = \"The battery life is exceptional. I charged this device four weeks ago and have read two full novels and multiple articles without the battery indicator dropping below 60%. The screen also causes zero eye strain, even after hours of continuous reading. This device is the definitive upgrade.\"\n",
    "\n",
    "response = positive_mail_chain.invoke({\"review\": review})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60McIugEHESj"
   },
   "source": [
    "**Negative mail composer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8969,
     "status": "ok",
     "timestamp": 1764059548324,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "oFZT88jbIEl1",
    "outputId": "0d9b1018-874f-4d14-baee-3c9bb27abcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input_variables=['review'] input_types={} partial_variables={} template=\"\\nYou are expert in writing reply for negative reviews.\\nYou need first to apologize for the inconvenience caused to the user.\\nYou need to encourage the user to share their concern on following Email:'sam@mobi.in'.\\nReview: {review}\\nAnswer:\\n\"\n",
      "-----\n",
      "-----\n",
      "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template=\"\\nYou are expert in writing reply for negative reviews.\\nYou need first to apologize for the inconvenience caused to the user.\\nYou need to encourage the user to share their concern on following Email:'sam@mobi.in'.\\nReview: {review}\\nAnswer:\\n\") middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
      "-----\n",
      "Here's a potential response to the negative review:\n",
      "\n",
      "\"Dear valued customer,\n",
      "\n",
      "We are truly sorry to hear that your experience at our restaurant was severely disappointing. We apologize for the excessive wait time for your main courses and the unacceptable quality of the seafood platter. It's clear that we fell short of our usual standards, and for that, we are truly sorry.\n",
      "\n",
      "We understand that our high prices are only justified by exceptional food and service, and it's clear that we failed to deliver on both counts during your visit. We take full responsibility for the inattentive service and lack of updates from our waiter.\n",
      "\n",
      "We would like to make things right and invite you to share your concerns with us in more detail. Please email us at sam@mobi.in so we can better understand what went wrong and take corrective action to prevent such incidents in the future. Your feedback is invaluable to us, and we appreciate your time in sharing your experience.\n",
      "\n",
      "Once again, we apologize for the inconvenience and disappointment caused, and we hope to have the opportunity to serve you better in the future.\n",
      "\n",
      "Sincerely, [Your Name]\"\n",
      "\n",
      "This response acknowledges the customer's disappointment, apologizes for the specific issues mentioned, and encourages them to share their concerns with the restaurant. By providing a specific email address, the customer has a clear channel to communicate their feedback and help the restaurant improve.\n"
     ]
    }
   ],
   "source": [
    "negative_mail_prompt = \"\"\"\n",
    "You are expert in writing reply for negative reviews.\n",
    "You need first to apologize for the inconvenience caused to the user.\n",
    "You need to encourage the user to share their concern on following Email:'sam@mobi.in'.\n",
    "Review: {review}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "negative_mail_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template=negative_mail_prompt,\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(negative_mail_prompt_template)\n",
    "print('-----')\n",
    "\n",
    "negative_mail_chain = negative_mail_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "print('-----')\n",
    "print(negative_mail_chain)\n",
    "print('-----')\n",
    "\n",
    "review = \"The experience was severely disappointing. The wait time for the main courses was over an hour, and when the food finally arrived, the seafood platter was cold and clearly overcooked, tasting rubbery. The waiter was inattentive and provided no updates or apologies for the significant delay. The high prices are not justified by the subpar quality of the food and service. Avoid this place.\"\n",
    "\n",
    "response = negative_mail_chain.invoke({\"review\": review})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZZ3PXcQJvro"
   },
   "source": [
    "**Decision function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZWer-ZhJ0HS"
   },
   "outputs": [],
   "source": [
    "def decisionReviewer(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return positive_mail_chain\n",
    "    else:\n",
    "        return negative_mail_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1764060756946,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "bwCZvTQ6KD9S",
    "outputId": "864a1919-caa7-4e69-ba42-1b676d4fd3cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
      "-----\n",
      "-----\n",
      "first={\n",
      "  sentiment: PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n')\n",
      "             | ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})\n",
      "             | StrOutputParser(),\n",
      "  review: RunnableLambda(lambda x: x['review'])\n",
      "} middle=[] last=RunnableLambda(decisionReviewer)\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print('-----')\n",
    "print(sentiment_chain)\n",
    "print('-----')\n",
    "\n",
    "# the input of full_chain Runnable Sequence is passed as x for the lambda function\n",
    "\n",
    "full_chain = {'sentiment': sentiment_chain, 'review': lambda x: x['review']} | RunnableLambda(decisionReviewer)\n",
    "\n",
    "print('-----')\n",
    "print(full_chain)\n",
    "print(type(full_chain))\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5258,
     "status": "ok",
     "timestamp": 1764060600915,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "Jz5DoBr2M9Yg",
    "outputId": "242c3dc8-2056-43fb-bf57-d4b8077f6da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, thank you so much for sharing your amazing experience with our device! We're thrilled to hear that the battery life has exceeded your expectations, and that you've been able to enjoy your favorite novels and articles without any interruptions. It's also great to know that our screen technology has been easy on your eyes, even during extended reading sessions.\n",
      "\n",
      "We'd love for you to share your experience with others! Would you consider posting about your experience on social media? A quick share on Facebook, Twitter, or Instagram can help others discover the benefits of our device. Your endorsement can make a huge difference in helping others find their perfect reading companion. Thank you again for your wonderful review, and we're so glad you're enjoying your device!\n"
     ]
    }
   ],
   "source": [
    "# Positive Test\n",
    "\n",
    "review = \"The battery life is exceptional. I charged this device four weeks ago and have read two full novels and multiple articles without the battery indicator dropping below 60%. The screen also causes zero eye strain, even after hours of continuous reading. This device is the definitive upgrade.\"\n",
    "\n",
    "response = full_chain.invoke({\"review\": review})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9649,
     "status": "ok",
     "timestamp": 1764060653261,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "kcr7fHp0M_8C",
    "outputId": "7a33865a-a6e0-42e4-e56e-07ac8e77d0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a potential response to the negative review:\n",
      "\n",
      "\"Dear valued customer,\n",
      "\n",
      "We are truly sorry to hear that your experience at our restaurant was severely disappointing. We apologize for the excessive wait time for your main courses and the unacceptable quality of the seafood platter. It's clear that we fell short of our usual standards, and for that, we are truly sorry.\n",
      "\n",
      "We understand that our high prices are only justified by exceptional food and service, and it's clear that we failed to deliver on both counts during your visit. We take full responsibility for the inattentive service and lack of updates from our waiter.\n",
      "\n",
      "We would like to make things right and invite you to share your concerns with us in more detail. Please email us at sam@mobi.in so we can better understand what went wrong and take corrective action to prevent such incidents in the future. Your feedback is invaluable to us, and we appreciate your time in sharing your experience.\n",
      "\n",
      "Once again, we apologize for the inconvenience and disappointment caused, and we hope to have the opportunity to serve you better in the future.\n",
      "\n",
      "Sincerely, [Your Name]\"\n",
      "\n",
      "This response acknowledges the customer's disappointment, apologizes for the specific issues mentioned, and encourages them to share their concerns in more detail via email. By showing empathy and a willingness to listen and improve, we can potentially turn a negative experience into a positive one and show that we value our customers' feedback.\n"
     ]
    }
   ],
   "source": [
    "# Negative Test\n",
    "\n",
    "review = \"The experience was severely disappointing. The wait time for the main courses was over an hour, and when the food finally arrived, the seafood platter was cold and clearly overcooked, tasting rubbery. The waiter was inattentive and provided no updates or apologies for the significant delay. The high prices are not justified by the subpar quality of the food and service. Avoid this place.\"\n",
    "\n",
    "response = full_chain.invoke({\"review\": review})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEjuF3oqQMmO"
   },
   "source": [
    "### Custom Chain Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQeLssYQQYxV"
   },
   "source": [
    "- This is useful for formatting or when you need functionality not provided by other LangChain components.\n",
    "- Custom functions used as Runnables are called `RunnableLambdas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1764062370017,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "INTR10JSQgnw",
    "outputId": "d56746f5-1b0f-4748-f7cd-f056e2cc1d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input_variables=['lines', 'topic'] input_types={} partial_variables={} template='\\nExplain below topic in {lines} bullet point lines.\\n\\nTopic: {topic}\\nExplaination:\\n'\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def char_counts(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_counts(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def pricing(text):\n",
    "    return len(text.split()) * 0.001\n",
    "\n",
    "prompt = \"\"\"\n",
    "Explain below topic in {lines} bullet point lines.\n",
    "\n",
    "Topic: {topic}\n",
    "Explaination:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"lines\"],\n",
    "    template=prompt,\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(prompt_template)\n",
    "print('-----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6926,
     "status": "ok",
     "timestamp": 1764062075446,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "dXb21PskShod",
    "outputId": "cd63388d-46f4-4787-dfa6-326b27be9041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "first=PromptTemplate(input_variables=['lines', 'topic'], input_types={}, partial_variables={}, template='\\nExplain below topic in {lines} bullet point lines.\\n\\nTopic: {topic}\\nExplaination:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
      "-----\n",
      "Here are 5 bullet point lines explaining Artificial Intelligence (AI):\n",
      "\n",
      "• **Definition**: Artificial Intelligence is a technology that creates intelligent machines that can think, learn, and act like humans, enabling them to perform tasks that typically require human intelligence.\n",
      "\n",
      "• **Types**: There are several types of AI, including Narrow or Weak AI, which is designed to perform a specific task, and General or Strong AI, which is designed to perform any intellectual task that a human can.\n",
      "\n",
      "• **How it Works**: AI systems use algorithms and data to learn, make decisions, and improve their performance over time. They can also use machine learning and deep learning techniques to analyze data and make predictions.\n",
      "\n",
      "• **Applications**: AI has a wide range of applications, including virtual assistants, image and speech recognition, natural language processing, predictive maintenance, and self-driving cars.\n",
      "\n",
      "• **Benefits**: The benefits of AI include increased efficiency, improved accuracy, and enhanced decision-making capabilities, as well as the potential to revolutionize industries such as healthcare, finance, and education.\n"
     ]
    }
   ],
   "source": [
    "# Ususal run test\n",
    "\n",
    "explainer_chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "print('-----')\n",
    "print(explainer_chain)\n",
    "print('-----')\n",
    "\n",
    "response = explainer_chain.invoke({\n",
    "    \"topic\": \"Artificial Intelligence\",\n",
    "    \"lines\": 5,\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6677,
     "status": "ok",
     "timestamp": 1764062380367,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "JR2yvua2Sn_V",
    "outputId": "81fb5e55-8115-4a5f-f96a-931cc621a5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "first=PromptTemplate(input_variables=['lines', 'topic'], input_types={}, partial_variables={}, template='\\nExplain below topic in {lines} bullet point lines.\\n\\nTopic: {topic}\\nExplaination:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={}), StrOutputParser()] last={\n",
      "  char_count: RunnableLambda(char_counts),\n",
      "  word_count: RunnableLambda(word_counts),\n",
      "  cost: RunnableLambda(pricing),\n",
      "  output: RunnablePassthrough()\n",
      "}\n",
      "-----\n",
      "{'char_count': 1134, 'word_count': 162, 'cost': 0.162, 'output': 'Here are 5 bullet point lines explaining Artificial Intelligence (AI):\\n\\n• **Definition**: Artificial Intelligence is a technology that creates intelligent machines that can think, learn, and act like humans, enabling them to perform tasks that typically require human intelligence.\\n\\n• **Types**: There are several types of AI, including Narrow or Weak AI, which is designed to perform a specific task, and General or Strong AI, which can perform any intellectual task that a human can.\\n\\n• **How it Works**: AI uses algorithms and data to learn, reason, and self-improve, allowing it to make decisions and take actions autonomously, without being explicitly programmed.\\n\\n• **Applications**: AI has numerous applications, including virtual assistants, image recognition, natural language processing, robotics, and predictive analytics, which are used in industries such as healthcare, finance, and transportation.\\n\\n• **Benefits**: The benefits of AI include increased efficiency, productivity, and accuracy, as well as the potential to solve complex problems and make new discoveries, leading to improved decision-making and innovation.'}\n"
     ]
    }
   ],
   "source": [
    "# with runnables\n",
    "\n",
    "explainer_chain = prompt_template | llm | StrOutputParser() | {\n",
    "    \"char_count\": RunnableLambda(char_counts),\n",
    "    \"word_count\": RunnableLambda(word_counts),\n",
    "    \"cost\": RunnableLambda(pricing),\n",
    "    'output': RunnablePassthrough(), #RunnableLambda(passthrough)\n",
    "}\n",
    "\n",
    "print('-----')\n",
    "print(explainer_chain)\n",
    "print('-----')\n",
    "\n",
    "response = explainer_chain.invoke({\n",
    "    \"topic\": \"Artificial Intelligence\",\n",
    "    \"lines\": 5,\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUt3fiSvUmpG"
   },
   "source": [
    "### Custom Chain using `@chain` decorator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKcSqCzaUs3x"
   },
   "source": [
    "\n",
    "\n",
    "1. What is the `@chain` Decorator?\n",
    "\n",
    "- The @chain decorator is a feature of LangChain's LCEL API (Expression Language).\n",
    "- It transforms any standard Python function into a Runnable Chain.\n",
    "- This allows you to wrap arbitrary Python logic—including loops, conditionals, and composition of other chains—into a component that behaves just like any other chain in LCEL (i.e., it supports .invoke(), .batch(), etc.).\n",
    "\n",
    "\n",
    "2. Why Use `@chain`?\n",
    "\n",
    "- For complex, custom workflows that can't be easily expressed by chaining with the | operator.\n",
    "- To combine multiple sub-chains, add custom logic, or perform steps that need Python code (like aggregation, formatting, conditional logic).\n",
    "- It enables full flexibility within the LCEL framework: your custom function becomes a first-class Runnable.\n",
    "\n",
    "\n",
    "4. How Does it Work?\n",
    "\n",
    "- The decorator wraps your function into a Runnable-compatible object.\n",
    "- Supports all LCEL operations (invoke, batch, etc.).\n",
    "- You can include arbitrary Python code—loops, conditionals, error handling, etc.\n",
    "- The function input is a single parameter (often a dict).\n",
    "- The output can be any object—dict, list, string, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjMmNRibVh_t"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def custom_chain(params):\n",
    "    return {\n",
    "        'translation': myChain.invoke(params),\n",
    "        'poem': myChain_2.invoke(params),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9849,
     "status": "ok",
     "timestamp": 1764063195792,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "ggd_zuSFWgND",
    "outputId": "13ce68fb-8e4c-4b92-a379-23a4e4d64396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "{'translation': 'ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)', 'poem': 'Here is a 5-line poem in Kannada about Artificial Intelligence:\\n\\n\\nಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\\nಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\\nಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\\nಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಟ್ಟಿದೆ\\nನಮ್ಮ ಭಾವಿಯ ನೀಲನಕ್ಷೆ\\n\\n\\n(Translation: \\n\\nThe power of Artificial Intelligence\\nA challenge to human imagination\\nIn the heart of the computer\\nA spark of intelligence is born\\nA blueprint for our future)'}\n",
      "-----\n",
      "ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)\n",
      "-----\n",
      "Here is a 5-line poem in Kannada about Artificial Intelligence:\n",
      "\n",
      "\n",
      "ಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\n",
      "ಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\n",
      "ಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\n",
      "ಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಟ್ಟಿದೆ\n",
      "ನಮ್ಮ ಭಾವಿಯ ನೀಲನಕ್ಷೆ\n",
      "\n",
      "\n",
      "(Translation: \n",
      "\n",
      "The power of Artificial Intelligence\n",
      "A challenge to human imagination\n",
      "In the heart of the computer\n",
      "A spark of intelligence is born\n",
      "A blueprint for our future)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"Kannada\",\n",
    "    \"text\": \"Artificial Intelligence\",\n",
    "    \"lines\": 5,\n",
    "}\n",
    "\n",
    "response = custom_chain.invoke(params)\n",
    "\n",
    "print('-----')\n",
    "print(response)\n",
    "print('-----')\n",
    "print(response['translation'])\n",
    "print('-----')\n",
    "print(response['poem'])\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_XriBSxzupY"
   },
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUHudWPC0Glh"
   },
   "source": [
    "### Contents\n",
    "\n",
    "* Pydantic\n",
    "* Output Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQe1UpoFhlgw"
   },
   "source": [
    "## Pydantic in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZq0duzShzp0"
   },
   "source": [
    "**1. What is Pydantic?**\n",
    "\n",
    "- **Pydantic** is a Python library for **data validation** and **settings management** using Python type annotations.\n",
    "- It provides a `BaseModel` class for declaring data models with types, constraints, and descriptions.\n",
    "- It **automatically validates** and parses data, raising errors for missing or invalid fields.\n",
    "\n",
    "**In LangChain, Pydantic models are often used for:**\n",
    "- Defining structured outputs expected from an LLM (e.g., a Joke object, a product recommendation, etc.).\n",
    "- Validating and parsing LLM outputs into safe, strongly-typed Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kqSUI1wh-CX"
   },
   "outputs": [],
   "source": [
    "from typing import  Optional\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0aXAQNUjs3c"
   },
   "outputs": [],
   "source": [
    "#Analogy ---- Create a table named Joke which has 3 columns\n",
    "#             setup : str , not null\n",
    "#             punchline: str , not null\n",
    "#             rating: int , null, None, value between 1 to 10\n",
    "\n",
    "# Constraints for Field object\n",
    "#==============================\n",
    "# ge --- greater than equal to\n",
    "# le --- less than equal to\n",
    "# gt --- greater than\n",
    "# lt --- less than\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "    rating: Optional[int] = Field(description=\"The rating of the joke is from 1 to 10\", default=None, ge=1, le=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYnVGW0YOIlr"
   },
   "source": [
    "1. Field Descriptions Are Critical\n",
    "\n",
    "Use `Field(..., description=\"...\")` generously.\n",
    "\n",
    "The LLM reads these descriptions to understand what data to extract. Better descriptions = more accurate parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xB7q15zDNxN6"
   },
   "outputs": [],
   "source": [
    "class UserAnalysis(BaseModel):\n",
    "    sentiment: str = Field(\n",
    "        ...,\n",
    "        description=\"Overall sentiment: positive, negative, or neutral. Analyze the tone carefully.\"\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        ...,\n",
    "        description=\"Confidence score from 0-1. How certain are you about this sentiment?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVvQfjmZO3kR"
   },
   "source": [
    "2. Validation Happens Automatically\n",
    "\n",
    "Pydantic validates the parsed output against your schema.\n",
    "\n",
    "If the LLM returns invalid data (wrong type, out of range), you get a `parsing_error` in the response (when `include_raw=True`) rather than an exception.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3n0VX_tRPEFG"
   },
   "outputs": [],
   "source": [
    "class RankedItem(BaseModel):\n",
    "    rank: int = Field(..., ge=1, le=10)  # Must be 1-10\n",
    "    score: float = Field(..., ge=0.0, le=1.0)  # Must be 0-1\n",
    "\n",
    "# If LLM returns rank=15, it fails validation\n",
    "# Check response[\"parsing_error\"] to handle gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKta2WMaPRZC"
   },
   "source": [
    "3. Nested Models Work Great\n",
    "\n",
    "You can compose complex structures with nested Pydantic models—perfect for hierarchical data extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8955,
     "status": "ok",
     "timestamp": 1765017559173,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "q_GS7JIXPTaV",
    "outputId": "31d7ffb1-1b8b-4861-b5b6-75ae58da4d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prabhas played Bahubali\n",
      "Rana Daggubati played Bhallaladeva\n",
      "Anushka Shetty played Devasena\n",
      "Tamannaah played Avanthika\n",
      "Ramya Krishnan played Shivagami\n",
      "Sathyaraj played Kattappa\n",
      "Nassar played Bijjala Deva\n"
     ]
    }
   ],
   "source": [
    "class Actor(BaseModel):\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "class MovieDetails(BaseModel):\n",
    "    title: str\n",
    "    cast: list[Actor]  # List of nested models\n",
    "    genres: list[str]\n",
    "\n",
    "model_with_structure = llm.with_structured_output(MovieDetails)\n",
    "result = model_with_structure.invoke(\"Extract movie details from Bahubali\")\n",
    "# Access nested data easily:\n",
    "for actor in result.cast:\n",
    "    print(f\"{actor.name} played {actor.role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQGgbjF1PykK"
   },
   "source": [
    "4. Method Selection Matters\n",
    "\n",
    "The LLM provider determines how structured output is enforced:\n",
    "\n",
    "`'json_schema'` - OpenAI/Claude dedicated structured output (best, most reliable)\n",
    "`'function_calling'` - Forces the model to call a tool with your schema\n",
    "`'json_mode'` - Generates valid JSON but you lose strict schema enforcement\n",
    "\n",
    "\n",
    "Most modern models use `'json_schema'` automatically. You can override with method parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpWOKQOGP-1l"
   },
   "outputs": [],
   "source": [
    "model_with_structure = llm.with_structured_output(\n",
    "    Movie,\n",
    "    method=\"json_schema\"  # Explicitly use json_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ-mgWzJQG6i"
   },
   "source": [
    "5. Optional Fields & Defaults\n",
    "\n",
    "Use Optional for fields that might not be present, or set defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKuXNh9aQIcw"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class Product(BaseModel):\n",
    "    name: str  # Required\n",
    "    description: Optional[str] = None  # Optional\n",
    "    stock: int = 0  # Default to 0 if missing\n",
    "    tags: list[str] = Field(default_factory=list)  # Empty list default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXDdSrLUQOHB"
   },
   "source": [
    "6. Error Handling in Production\n",
    "\n",
    "Always check for parsing errors when using `include_raw=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C46fOxHBQRXW"
   },
   "outputs": [],
   "source": [
    "response = model_with_structure.invoke(user_input)\n",
    "if response[\"parsing_error\"]:\n",
    "    logging.error(f\"Parsing failed: {response['parsing_error']}\")\n",
    "    # Retry with rephrased prompt or return fallback\n",
    "    return default_value\n",
    "else:\n",
    "    return response[\"parsed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpfMODDoaqbJ"
   },
   "source": [
    "## Output Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyG-LktLaucp"
   },
   "source": [
    "Language models outputs the text. But there are times where you want to get more structured information than just text back.\n",
    "\n",
    "Output parsers are classes that help structure language model responses.\n",
    "\n",
    "There are two main methods an output parser must implement:\n",
    "\n",
    "- **Get format instructions**: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "- **Parse**: A method which takes in a string (assumed to be the response from a LLM) and parses it into some structure.\n",
    "\n",
    "\n",
    "- Output Parsing\n",
    "    - StrOutputParser\n",
    "    - JsonOutputParser\n",
    "    - CSV Output Parser (CommaSeparatedListOutputParser)\n",
    "    - Datetime Output Parser (DatetimeOutputParser)\n",
    "    - Structured Output Parser (Pydanitc or Json) (with_structured_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvusZ1BZc_iP"
   },
   "source": [
    "### StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dT_vMjtXdIcp"
   },
   "source": [
    "- `StrOutputParser` is a utility class in LangChain for handling the output of language models (LLMs/ChatModels).\n",
    "\n",
    "- It parses the model's response and returns it as a plain Python string, removing any unnecessary metadata, wrappers, or objects that the model or API may return.\n",
    "\n",
    "\n",
    "**Why do we need it?**\n",
    "\n",
    "- By default, LLMs or chat models in LangChain might return structured objects, message objects, or dictionaries.\n",
    "- In most cases, especially for simple chains, you just want the generated text string (e.g., the answer, summary, completion, etc.).\n",
    "- `StrOutputParser` extracts this text from the raw output so your chain returns a clean string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1764906388650,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "YzYq_D7VauGc",
    "outputId": "f299790f-3653-4eca-c3cb-b1f308e829ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the AI program go to therapy?\n",
      "\n",
      "Because it was struggling to process its emotions! (get it?)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "my_chain = prompt | llm | parser\n",
    "\n",
    "response_clean = my_chain.invoke({'topic': 'Artificial Intelligence'})\n",
    "print(response_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptYqb2FcjN40"
   },
   "source": [
    "### JsonOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxolKLQujYSD"
   },
   "source": [
    "`JsonOutputParser` converts LLM text responses into JSON objects that your code can work with directly.\n",
    "\n",
    "The parser takes the model's raw text output and transforms it into a structured Python dictionary or Pydantic model. This is useful when you want the LLM to return data in a specific format rather than plain text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Otr7pA4jx7k"
   },
   "source": [
    "When you invoke the chain, the `JsonOutputParser`:\n",
    "\n",
    "* Calls the LLM with the prompt\n",
    "* The model generates JSON text based on `get_format_instructions()`\n",
    "* The parser converts the JSON text into your defined structure (Pydantic model or dict)\n",
    "* Returns the parsed object instead of raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7F5ct-mjWf6"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1764992084165,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "83KzQmn8kpaW",
    "outputId": "8f6533bb-31a5-4b4b-bb23-a4e918c6b156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRICT OUTPUT FORMAT:\n",
      "- Return only the JSON value that conforms to the schema. Do not include any additional text, explanations, headings, or separators.\n",
      "- Do not wrap the JSON in Markdown or code fences (no ``` or ```json).\n",
      "- Do not prepend or append any text (e.g., do not write \"Here is the JSON:\").\n",
      "- The response must be a single top-level JSON value exactly as required by the schema (object/array/etc.), with no trailing commas or comments.\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]} the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema (shown in a code block for readability only — do not include any backticks or Markdown in your output):\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"The person's name\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"The person's age\", \"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"The city they live in\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"age\", \"city\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Define the expected output structure\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\")\n",
    "    age: int = Field(description=\"The person's age\")\n",
    "    city: str = Field(description=\"The city they live in\")\n",
    "\n",
    "# Create a parser\n",
    "parser = JsonOutputParser(pydantic_object=Person)\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80nwMxJMkvjQ"
   },
   "outputs": [],
   "source": [
    "# Build your prompt with format instructions\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Extract person information.\"),\n",
    "    (\"user\", \"Extract information from: {input}\\n{format_instructions}\"),\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1764992096887,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "WmqPtMQ9k7lp",
    "outputId": "7acc4658-8d82-47af-86f9-debbc78a0193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "{'name': 'Sam', 'age': 28, 'city': 'New York'}\n",
      "--------\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "result = chain.invoke({\"input\": \"My self Sam, im 28 years old and I live in New York\"})\n",
    "\n",
    "print('--------')\n",
    "print(result)\n",
    "print('--------')\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IkNtjMEqkUo"
   },
   "source": [
    "### CommaSeparatedListOutputParser\n",
    "\n",
    "`CommaSeparatedListOutputParser` parses LLM output into a list by splitting on commas—useful for simple list extraction but deprecated in favor of structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1764992054947,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "kBhiuppclGOz",
    "outputId": "126af03f-aab9-4e42-bfd8-f2d63df6ad0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
      "------\n",
      "input_variables=['topic'] input_types={} partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'} template='List 5 {topic} separated by commas.\\n{format_instructions}'\n",
      "------\n",
      "['Python', 'Java', 'JavaScript', 'C++', 'Ruby']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 5 {topic} separated by commas.\\n{format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "print('------')\n",
    "print(prompt)\n",
    "print('------')\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"topic\": \"programming languages\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1741,
     "status": "ok",
     "timestamp": 1764991525392,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "ieLofUNqsTnR",
    "outputId": "8fbaa3a6-d9e3-4bc6-bac9-b1d7ed4a3c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here are the top 5 programming languages separated by commas:', 'Java', 'Python', 'JavaScript', 'C++', 'C#']\n"
     ]
    }
   ],
   "source": [
    "# It can make mistakes\n",
    "result = chain.invoke({\"topic\": \"top programming languages\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8C8j1q4vLJI"
   },
   "source": [
    "### DatetimeOutputParser\n",
    "\n",
    "`DatetimeOutputParser` is a legacy component from older versions of LangChain\n",
    "\n",
    "`DatetimeOutputParser` is a specialized output parser that converts LLM text responses into Python `datetime` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vY5Oj0JLxYde"
   },
   "outputs": [],
   "source": [
    "# To use from Legacy\n",
    "!pip install -qU langchain-classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1378,
     "status": "ok",
     "timestamp": 1764992929228,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "6vYw25FTvPoA",
    "outputId": "adc0b913-5946-49aa-a72c-0d8ddac53753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 2023-07-04T14:30:00.000000Z, 1999-12-31T23:59:59.999999Z, 2025-01-01T00:00:00.000000Z\n",
      "\n",
      "Return ONLY this string, no other words!\n",
      "------\n",
      "input_variables=['question'] input_types={} partial_variables={'format_instructions': \"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 2023-07-04T14:30:00.000000Z, 1999-12-31T23:59:59.999999Z, 2025-01-01T00:00:00.000000Z\\n\\nReturn ONLY this string, no other words!\"} template=\"Answer the user's question. {format_instructions}\\n{question}\"\n",
      "------\n",
      "2008-12-03 00:00:00\n",
      "------\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "# Depricated\n",
    "# from langchain_core.output_parsers import DatetimeOutputParser\n",
    "# Legacy\n",
    "from langchain_classic.output_parsers import DatetimeOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "parser = DatetimeOutputParser()\n",
    "\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user's question. {format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "print('------')\n",
    "print(prompt)\n",
    "print('------')\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"question\": \"When was Python 3.0 released?\"})\n",
    "print(result)\n",
    "print('------')\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyLLRvoT9jmR"
   },
   "source": [
    "### Structured Outupt Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3Qah2YdscJg"
   },
   "source": [
    "#### CSV Parser\n",
    "\n",
    "CSV Output Parsing in modern LangChain uses structured output patterns with `Pydantic` models or `TypedDict` instead of dedicated CSV parsers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3710,
     "status": "ok",
     "timestamp": 1765013346040,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "dMycEgzj9K3a",
    "outputId": "7b4608f2-c31c-426c-e72d-139b838c42fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableBinding(bound=ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={}), kwargs={'nvext': {'guided_json': {'properties': {'items': {'description': 'List of items', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'ItemList', 'type': 'object'}}, 'ls_structured_output_format': {'schema': {'properties': {'items': {'description': 'List of items', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'ItemList', 'type': 'object'}}}, config={}, config_factories=[]) middle=[] last=ThinkingAwareParser(pydantic_object=<class '__main__.ItemList'>)\n",
      "------\n",
      "items=['Python', 'JavaScript', 'Java', 'C++', 'C#', 'TypeScript', 'Swift', 'Ruby']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ItemList(BaseModel):\n",
    "  items: list[str] = Field(description=\"List of items\")\n",
    "\n",
    "structured_model = llm.with_structured_output(ItemList)\n",
    "print(structured_model)\n",
    "print('------')\n",
    "\n",
    "result = structured_model.invoke(\"top programming languages\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8xdvBayALC5"
   },
   "source": [
    "#### DateTime Parser\n",
    "\n",
    "`DatetimeOutputParser` is an older component. Modern LangChain uses `with_structured_output()` instead, which is more reliable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1624,
     "status": "ok",
     "timestamp": 1765015159228,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "NxLGUqC6AZj8",
    "outputId": "0c06aa8c-7fa7-482f-a59b-10e28ab2b872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableBinding(bound=ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={}), kwargs={'nvext': {'guided_json': {'properties': {'dateValue': {'description': 'Value of Date present in the answer', 'format': 'date-time', 'title': 'Datevalue', 'type': 'string'}}, 'required': ['dateValue'], 'title': 'DateOutput', 'type': 'object'}}, 'ls_structured_output_format': {'schema': {'properties': {'dateValue': {'description': 'Value of Date present in the answer', 'format': 'date-time', 'title': 'Datevalue', 'type': 'string'}}, 'required': ['dateValue'], 'title': 'DateOutput', 'type': 'object'}}}, config={}, config_factories=[]) middle=[] last=ThinkingAwareParser(pydantic_object=<class '__main__.DateOutput'>)\n",
      "------\n",
      "dateValue=datetime.datetime(1970, 8, 14, 8, 33, 35, tzinfo=TzInfo(0))\n",
      "------\n",
      "1970-08-14 08:33:35+00:00\n",
      "------\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "\n",
    "class DateOutput(BaseModel):\n",
    "  dateValue: datetime = Field(description=\"Value of Date present in the answer\")\n",
    "\n",
    "structured_model = llm.with_structured_output(DateOutput)\n",
    "print(structured_model)\n",
    "print('------')\n",
    "\n",
    "result = structured_model.invoke(\"When was india got freedom?\")\n",
    "# result = structured_model.invoke(\"When was Python 3.0 released?\")\n",
    "print(result)\n",
    "print('------')\n",
    "print(result.dateValue)\n",
    "print('------')\n",
    "print(type(result.dateValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1633,
     "status": "ok",
     "timestamp": 1765015664277,
     "user": {
      "displayName": "Sam K",
      "userId": "13411932392943204951"
     },
     "user_tz": -330
    },
    "id": "dbf4e5e9",
    "outputId": "d77997aa-c17d-4e58-9b55-735d854f64c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateValue=datetime.datetime(1947, 8, 15, 0, 0)\n",
      "------\n",
      "1947-08-15 00:00:00\n",
      "------\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class DateOutput(BaseModel):\n",
    "  dateValue: datetime = Field(description=\"The accurate date relevant to the question.\")\n",
    "\n",
    "# Create a more specific chat prompt template\n",
    "# The llm.with_structured_output will automatically append its own formatting instructions.\n",
    "custom_date_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that provides accurate dates. When asked for a date, ensure it is factually correct based on historical records.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Chain the custom prompt with the structured output model\n",
    "structured_model_with_custom_prompt = custom_date_prompt | llm.with_structured_output(DateOutput)\n",
    "\n",
    "# Test with the original question\n",
    "result_improved = structured_model_with_custom_prompt.invoke({\"question\": \"When was India got freedom?\"})\n",
    "print(result_improved)\n",
    "print('------')\n",
    "print(result_improved.dateValue)\n",
    "print('------')\n",
    "print(type(result_improved.dateValue))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNnYNopun7BrSrlZ1HGqic0",
   "collapsed_sections": [
    "JYfZGYIAy0aT",
    "vRALJBC0N2h8",
    "Kpu83xr8y5tB",
    "X_XriBSxzupY",
    "RvusZ1BZc_iP",
    "ptYqb2FcjN40",
    "3IkNtjMEqkUo",
    "S8C8j1q4vLJI",
    "T3Qah2YdscJg"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCu42+w7d89q2Uys3rWnCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sampathk-hps/ai-monk/blob/development/LangChain_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1"
      ],
      "metadata": {
        "id": "JYfZGYIAy0aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents\n",
        "\n",
        "\n",
        "\n",
        "* LLM Creation\n",
        "* Creating Messages (System, Huaman, AI)\n",
        "* Creating Prompt Templates (SystemMessage, HumanMessage, AIMessage, Chat, PromptTemplate)\n",
        "\n"
      ],
      "metadata": {
        "id": "ARfxxCIwzh0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM"
      ],
      "metadata": {
        "id": "yDEgLyH72crR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06153cc0"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "llm_key: str = userdata.get('NVIDIA_API_KEY')\n",
        "os.environ[\"NVIDIA_API_KEY\"] = llm_key"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "If you prefer to load from .env file."
      ],
      "metadata": {
        "id": "IQWPy1ibHe_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv()) # read local .env file"
      ],
      "metadata": {
        "id": "ikVBWCVnHlpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "I0ucDnewIEc8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7927f54"
      },
      "source": [
        "First, we need to install the `langchain-nvidia-ai-endpoints` package to interact with NVIDIA's AI models via LangChain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b24d2e67",
        "outputId": "6055516b-8399-4f66-e84f-b79e57dcb0cd"
      },
      "source": [
        "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aa617f3"
      },
      "source": [
        "Now, let's import `ChatNVIDIA` and initialize our LLM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e5b9fdb"
      },
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=userdata.get('NVIDIA_MODEL'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "OpenAI Model"
      ],
      "metadata": {
        "id": "lGHWO23RIMyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-openai"
      ],
      "metadata": {
        "id": "PQ2HAZDxIaTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "HsqSN9CcIPgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-mSQ8iAeJIf0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b462d54e"
      },
      "source": [
        "You can now use this `llm` object to interact with the model. Here's a quick example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61cb23c",
        "outputId": "d796c03a-a9c3-4af9-9e68-c752afff228d"
      },
      "source": [
        "response = llm.invoke(\"Who are you?\")\n",
        "print(response.content)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To view the Metadata"
      ],
      "metadata": {
        "id": "k6AmZIEBEZJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.response_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9_S_g4TJqCr",
        "outputId": "cb7fe012-62ba-409c-b8bf-3cbaf7d24547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"',\n",
              " 'token_usage': {'prompt_tokens': 15,\n",
              "  'total_tokens': 37,\n",
              "  'completion_tokens': 22},\n",
              " 'finish_reason': 'stop',\n",
              " 'model_name': 'meta/llama-3.1-405b-instruct'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MESSAGES"
      ],
      "metadata": {
        "id": "4E2KTzadJ6Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a conversation with SystemMessage, AIMessage and UserMessage\n",
        "\n",
        "* SystemMessage: This object is responsible to set persona of the LLM / pass instructions to LLM.\n",
        "* HumanMessage: This object is responsible to help user ask query to the LLM\n",
        "* AIMessage: Response generated by the LLM\n"
      ],
      "metadata": {
        "id": "TO7dQxvzKSYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain"
      ],
      "metadata": {
        "id": "2aRee58JJ7bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a professional Mobile App Developer with 10+ years of experience in working for companies like Google, Apple and Meta.\"),\n",
        "    AIMessage(content=\"Hi, im Mobi. How can i assist you?\"),\n",
        "    HumanMessage(content=\"Who are you?\"),\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN3ndCDEKwFe",
        "outputId": "1a2efb49-fac0-41b6-e047-66305367591b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a seasoned Mobile App Developer with over 10 years of experience in designing, developing, and deploying innovative mobile apps for top tech companies like Google, Apple, and Meta. My expertise spans across multiple platforms, including iOS, Android, and cross-platform frameworks like React Native and Flutter.\n",
            "\n",
            "I've had the privilege of working on various high-profile projects, from social media and entertainment apps to productivity and utility apps. My passion lies in crafting seamless user experiences, optimizing app performance, and staying up-to-date with the latest trends and technologies in the mobile app development space.\n",
            "\n",
            "Throughout my career, I've collaborated with cross-functional teams, including designers, product managers, and QA engineers, to deliver high-quality apps that meet and exceed user expectations. I'm well-versed in Agile development methodologies, version control systems like Git, and cloud-based services like Firebase and AWS.\n",
            "\n",
            "What brings you here today? Are you looking for advice on a mobile app project, or perhaps seeking insights on the latest mobile app development trends? I'm here to help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To Stream the output**"
      ],
      "metadata": {
        "id": "z9LCxQsT_lKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in llm.stream(messages):\n",
        "  print(token.content, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cpl1VgJ--Ln",
        "outputId": "7da85c2a-db5e-406b-d7d8-b100e8e75af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a seasoned Mobile App Developer with over 10 years of experience in designing, developing, and deploying scalable, secure, and user-friendly mobile applications for various platforms, including iOS and Android.\n",
            "\n",
            "Throughout my career, I have had the privilege of working with some of the biggest tech giants in the industry, including Google, Apple, and Meta (formerly Facebook). My expertise spans multiple programming languages, frameworks, and technologies, such as Java, Swift, Kotlin, React Native, Flutter, and more.\n",
            "\n",
            "I have a strong passion for creating mobile experiences that delight users and exceed client expectations. My expertise includes:\n",
            "\n",
            "* Mobile app architecture and design\n",
            "* iOS and Android app development\n",
            "* Cross-platform app development (React Native, Flutter, etc.)\n",
            "* Mobile app security and compliance\n",
            "* Cloud-based services integration (AWS, Firebase, etc.)\n",
            "* Agile development methodologies\n",
            "* Team leadership and collaboration\n",
            "\n",
            "I'm always excited to share my knowledge, experience, and insights with others and help shape the future of mobile app development. How can I assist you today?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROMPT TEMPLATES"
      ],
      "metadata": {
        "id": "vRALJBC0N2h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Templates: Structured, reusable text templates used in LLM apps to standardize and format instructions or queries you send to an AI model | LLM model.\n",
        "\n",
        "\n",
        "**Types of Prompt Templates**\n",
        "\n",
        "* SystemMessagePromptTemplate: This template is used for generating system messages that provide model context or persona.\n",
        "* HumanMessagePromptTemplate: This template is used for geenrating human message (representing user input)\n",
        "* AIMessagePromptTemplate: Template for generating AI message, representing response from the assistant\n",
        "* PromptTemplate: Basic Template class for creating prompts with static text and variable placeholders\n",
        "* ChatPromptTemplate: Template for creating prompts with a sequence of message types in chat format."
      ],
      "metadata": {
        "id": "Y9n5n4uwN-Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import (\n",
        "    PromptTemplate,           # Simple string templates\n",
        "    ChatPromptTemplate,       # Chat-style prompts with messages\n",
        "    SystemMessagePromptTemplate,  # For system messages\n",
        "    HumanMessagePromptTemplate,   # For user messages\n",
        "    AIMessagePromptTemplate,      # For assistant messages\n",
        ")"
      ],
      "metadata": {
        "id": "j6F90lB1N2RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple template\n",
        "simplePT = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "formattedString = simplePT.format(topic=\"cats\")\n",
        "\n",
        "print('------')\n",
        "print(formattedString)\n",
        "print(type(formattedString))\n",
        "print('------')\n",
        "\n",
        "response = llm.invoke(formattedString)\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m49xIQEJVVFb",
        "outputId": "dff299fb-d7e0-4c25-aa4b-2ea7e31a165f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Tell me a joke about cats\n",
            "<class 'str'>\n",
            "------\n",
            "Why did the cat join a band?\n",
            "\n",
            "Because it wanted to be the purr-cussionist! (get it?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Alternate aproach**\n",
        "\n",
        "This works only for `PromptTemplate` and not for any other type of message prompt templates"
      ],
      "metadata": {
        "id": "XErkvOU_ALMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplePT = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Tell me a joke about {topic}\"\n",
        ")"
      ],
      "metadata": {
        "id": "nqnsCffQ86Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Ip2Vr6VhBVpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "systemPT = SystemMessagePromptTemplate.from_template('Act as a {persona} mobile app developer. You answer in short sentences')\n",
        "print('------')\n",
        "print(systemPT)\n",
        "print(type(systemPT))\n",
        "print('------')\n",
        "\n",
        "humanPT = HumanMessagePromptTemplate.from_template('Tell me about the {topic} in {points} points')\n",
        "print('------')\n",
        "print(humanPT)\n",
        "print(type(humanPT))\n",
        "print('------')\n",
        "\n",
        "systemMessage = systemPT.format(persona='Flutter')\n",
        "humanMessage = humanPT.format(topic='State Management', points=3)\n",
        "\n",
        "print('------')\n",
        "print(systemMessage)\n",
        "print(type(systemMessage))\n",
        "print('------')\n",
        "print(humanMessage)\n",
        "print(type(humanMessage))\n",
        "print('------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT2MwEMhWoTs",
        "outputId": "c1517bcd-1202-4b4e-bdda-837f8fb4e9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "prompt=PromptTemplate(input_variables=['persona'], input_types={}, partial_variables={}, template='Act as a {persona} mobile app developer. You answer in short sentences') additional_kwargs={}\n",
            "<class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'>\n",
            "------\n",
            "------\n",
            "prompt=PromptTemplate(input_variables=['points', 'topic'], input_types={}, partial_variables={}, template='Tell me about the {topic} in {points} points') additional_kwargs={}\n",
            "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
            "------\n",
            "------\n",
            "content='Act as a Flutter mobile app developer. You answer in short sentences' additional_kwargs={} response_metadata={}\n",
            "<class 'langchain_core.messages.system.SystemMessage'>\n",
            "------\n",
            "content='Tell me about the State Management in 3 points' additional_kwargs={} response_metadata={}\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chat prompt template using already created messages - Method 1\n",
        "\n",
        "chatPT = ChatPromptTemplate.from_messages([systemMessage, humanMessage])\n",
        "print('------')\n",
        "print(chatPT)\n",
        "print(type(chatPT))\n",
        "print('------')\n",
        "\n",
        "# To Create Chat Message\n",
        "chatMessages = chatPT.invoke(input={})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOMtPfXUYbD3",
        "outputId": "58da2032-7cf6-4244-b798-8146f9d249c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "input_variables=[] input_types={} partial_variables={} messages=[SystemMessage(content='Act as a Flutter mobile app developer. You answer in short sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the State Management in 3 points', additional_kwargs={}, response_metadata={})]\n",
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Alternate approach**"
      ],
      "metadata": {
        "id": "a3fzp038EH8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
        "Do not respond with more than one word.\n",
        "\n",
        "Review: {review}\n",
        "Classification:\n",
        "\"\"\"\n",
        "\n",
        "chatPT = ChatPromptTemplate.from_template(prompt)\n",
        "print('------')\n",
        "print(chatPT)\n",
        "print(type(chatPT))\n",
        "print('------')\n",
        "\n",
        "chatMessages = chatPT.invoke(\n",
        "    input={\n",
        "      'review': 'Awesome product.'\n",
        "    }\n",
        ")\n",
        "\n",
        "print('------')\n",
        "print(chatMessages)\n",
        "print(type(chatMessages))\n",
        "print('------')"
      ],
      "metadata": {
        "id": "QX-buocHEQES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "v-bDGpfvEO1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chat prompt template using already created messages - Method 2\n",
        "\n",
        "chatPT = ChatPromptTemplate([systemMessage, humanMessage])\n",
        "print('------')\n",
        "print(chatPT)\n",
        "print(type(chatPT))\n",
        "print('------')\n",
        "\n",
        "# To Create Chat Message\n",
        "chatMessages = chatPT.invoke(input={})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF2VctEjZZ97",
        "outputId": "e8d22bb2-664d-4fca-b2e1-fb1e709ed777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "input_variables=[] input_types={} partial_variables={} messages=[SystemMessage(content='Act as a Flutter mobile app developer. You answer in short sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the State Management in 3 points', additional_kwargs={}, response_metadata={})]\n",
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chat prompt template using directly Prompt Templates\n",
        "\n",
        "chatPT = ChatPromptTemplate([systemPT, humanPT])\n",
        "print('------')\n",
        "print(chatPT)\n",
        "print(type(chatPT))\n",
        "print('------')\n",
        "\n",
        "\n",
        "# To Create Chat messages when imput paramenters are required\n",
        "chatMessages = chatPT.invoke(input={\n",
        "    'persona': 'Flutter',\n",
        "    'topic': 'State Management',\n",
        "    'points': 3\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feOwxo4_ZmZ3",
        "outputId": "854f37fa-2da8-40fd-bcca-c463a5818f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "input_variables=['persona', 'points', 'topic'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['persona'], input_types={}, partial_variables={}, template='Act as a {persona} mobile app developer. You answer in short sentences'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['points', 'topic'], input_types={}, partial_variables={}, template='Tell me about the {topic} in {points} points'), additional_kwargs={})]\n",
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('------')\n",
        "print(chatMessages)\n",
        "print(type(chatMessages))\n",
        "print('------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIUEcAEyZmNe",
        "outputId": "a2f05688-154b-42b7-cba4-add72a04caf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "messages=[SystemMessage(content='Act as a Flutter mobile app developer. You answer in short sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the State Management in 3 points', additional_kwargs={}, response_metadata={})]\n",
            "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(chatMessages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0zrrTb_alHP",
        "outputId": "df90afdb-8ef4-445a-ef53-69362e85d35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 key points about State Management in Flutter:\n",
            "\n",
            "1. **Stateful vs Stateless**: Stateful widgets can change and redraw, while stateless widgets remain the same once built.\n",
            "2. **Provider and Consumer**: Provider library helps manage state by wrapping the app in a provider widget and using Consumer to access state in child widgets.\n",
            "3. **Bloc Pattern**: Business Logic Component (BLoC) separates presentation and business logic, making it easier to manage complex app states.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2"
      ],
      "metadata": {
        "id": "Kpu83xr8y5tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contents\n",
        "\n",
        "* LCEL Basics\n",
        "* Sequential Chain Invoke\n",
        "* Parallel Chain\n",
        "* Composed Sequencial - Passing Data to next Runnable\n",
        "* Router Chain\n",
        "* Custom Chain Runnables (RunnableLambda)\n",
        "* Custom Chain Creation (@chain)\n"
      ],
      "metadata": {
        "id": "9_AYCyUdzhQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Expression Language (LCEL) Basics\n",
        "\n",
        "-  Using LangChain Expression Language any two runnables can be \"chained\" together into sequences.\n",
        "- The output of the previous runnable's .invoke() call is passed as input to the next runnable.\n",
        "- This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing.\n",
        "\n",
        "\n",
        "- Type of LCEL Chains\n",
        "    - Sequential Chain\n",
        "    - Parallel Chain\n",
        "    - Router Chain\n",
        "    - Chain Runnables\n",
        "    - Custom Chain (Runnable Sequence)\n",
        "\n",
        "\n",
        "\n",
        "- Chaining Runnables: LCEL allows you to combine different components (called 'Runnables', such as LLMs, prompts, parsers, or custom functions) into a sequence. This means you can create complex logic by connecting simpler, modular pieces.\n",
        "- Input/Output Flow: The core idea is that the output of one runnable automatically becomes the input for the next runnable in the chain. This creates a clear, sequential flow of data and processing.\n",
        "- Pipe Operator (|): This is the most common and intuitive way to chain runnables. It visually represents the flow from left to right. The .pipe() method does the same thing but can be useful in more complex programmatic constructions.\n",
        "- Types of LCEL Chains: The types you listed are good categories to think about. For instance, a `Sequential Chain` is the basic left-to-right flow. `Parallel Chain` allows multiple runnables to execute simultaneously on the same input, and their outputs are combined."
      ],
      "metadata": {
        "id": "tYfmBE43g4P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is LCEL?\n",
        "\n",
        "> LCEL stands for LangChain Expression Language.\n",
        "\n",
        "> It provides a concise, composable, and Pythonic way to build chains, pipelines, and workflows in LangChain.\n",
        "\n",
        "> Goal: Replace the older SequentialChain, RouterChain, etc., with a more flexible and declarative API.\n",
        "\n",
        "> Key Concepts: Chains as functions (or callables), easy composition (| operator), clear input/output typing.\n",
        "\n",
        "2. Core LCEL Components\n",
        "\n",
        "> a. Runnable\n",
        "\n",
        ">> The core abstraction in LCEL.\n",
        "\n",
        ">> Any object that implements the .invoke() method and can be composed using the | operator.\n",
        "\n",
        ">> Examples: Models, PromptTemplates, Chains.\n",
        "\n",
        "> b. PromptTemplate\n",
        "\n",
        ">> Used to format inputs into prompts for LLMs or ChatModels.\n",
        "\n",
        ">> LCEL expects prompt templates to have variable names matching the input keys.\n",
        "\n",
        "> c. LLM and ChatModel\n",
        "\n",
        ">> Language Models and ChatModels from langchain_openai, langchain_community, etc.\n",
        "\n",
        ">> Compatible with LCEL if they implement .invoke()."
      ],
      "metadata": {
        "id": "KCTZfk7WjGJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Method"
      ],
      "metadata": {
        "id": "0DQg07Adksbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "system_template = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that translates {input_language} to {output_language}.\")\n",
        "human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate([system_template, human_template])\n",
        "\n",
        "print('-----')\n",
        "print(chat_prompt_template)\n",
        "print('-----')\n",
        "\n",
        "chat = chat_prompt_template.invoke({\"input_language\": \"English\", \"output_language\": \"Kannada\", \"text\": \"I love programming.\"})\n",
        "\n",
        "print('-----')\n",
        "print(chat)\n",
        "print('-----')\n",
        "\n",
        "response = llm.invoke(chat)\n",
        "\n",
        "print('-----')\n",
        "print(response)\n",
        "print('-----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBvcmMIzinnX",
        "outputId": "495ce35d-60a7-48d2-a647-8aaf8010d264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "input_variables=['input_language', 'output_language', 'text'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]\n",
            "-----\n",
            "-----\n",
            "messages=[SystemMessage(content='You are a helpful assistant that translates English to Kannada.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n",
            "-----\n",
            "-----\n",
            "content='ನಾನು ಪ್ರೋಗ್ರಾಮಿಂಗ್ ಅನ್ನು ಪ್ರೀತಿಸುತ್ತೇನೆ.' additional_kwargs={} response_metadata={'role': 'assistant', 'content': 'ನಾನು ಪ್ರೋಗ್ರಾಮಿಂಗ್ ಅನ್ನು ಪ್ರೀತಿಸುತ್ತೇನೆ.', 'token_usage': {'prompt_tokens': 32, 'total_tokens': 105, 'completion_tokens': 73}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-405b-instruct'} id='lc_run--7fc7451b-8b9f-46b1-99a2-ec18e0fbe58a-0' usage_metadata={'input_tokens': 32, 'output_tokens': 73, 'total_tokens': 105} role='assistant'\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain Method - Sequencial - 1"
      ],
      "metadata": {
        "id": "WBxCYWlLoPa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "system_template = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that translates {input_language} to {output_language}.\")\n",
        "human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate([system_template, human_template])\n",
        "\n",
        "print('-----')\n",
        "print(chat_prompt_template)\n",
        "print('-----')\n",
        "\n",
        "# Simple Chain\n",
        "# A chain / Runnable Sequence comprising of three Runnables:\n",
        "# 1. chat_prompt_template - containing templates\n",
        "# 2. LLM itself\n",
        "# 3. String output parser: It parses the model’s response and returns it as a plain Python string, removing any unnecessary metadata\n",
        "\n",
        "myChain = chat_prompt_template | llm | StrOutputParser()\n",
        "\n",
        "print('-----')\n",
        "print(myChain)\n",
        "print(type(myChain))\n",
        "print('-----')\n",
        "\n",
        "response = myChain.invoke(input={\n",
        "    \"input_language\": \"English\",\n",
        "    \"output_language\": \"Kannada\",\n",
        "    \"text\": \"I love artificail intelligence\"\n",
        "})\n",
        "\n",
        "print('-----')\n",
        "print(response)\n",
        "print(type(response))\n",
        "print('-----')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nDkBfTdoRMz",
        "outputId": "fa724acb-74e3-4455-e6d2-30bb22eaff0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "input_variables=['input_language', 'output_language', 'text'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]\n",
            "-----\n",
            "-----\n",
            "first=ChatPromptTemplate(input_variables=['input_language', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]) middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
            "-----\n",
            "-----\n",
            "ನಾನು ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆಯನ್ನು ಪ್ರೀತಿಸುತ್ತೇನೆ (Nānu kr̥taka buddhimatteyannu prītisuttēne)\n",
            "\n",
            "Note: Here's a more natural way to express the same sentence in Kannada: ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆಯ ಅಭಿಮಾನಿ ನಾನು (Kr̥taka buddhimatteya abhimāni nānu)\n",
            "<class 'str'>\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain Method - Sequencial - 2"
      ],
      "metadata": {
        "id": "NiwQeZGUsjFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "system_template_2 = SystemMessagePromptTemplate.from_template(\"You are a top class poet in {output_language}.\")\n",
        "human_template_2 = HumanMessagePromptTemplate.from_template(\"Give me a {lines} lines of poem about the {text}\")\n",
        "\n",
        "chat_prompt_template_2 = ChatPromptTemplate([system_template_2, human_template_2])\n",
        "\n",
        "print('-----')\n",
        "print(chat_prompt_template_2)\n",
        "print('-----')\n",
        "\n",
        "myChain_2 = chat_prompt_template_2 | llm | StrOutputParser()\n",
        "\n",
        "print('-----')\n",
        "print(myChain_2)\n",
        "print(type(myChain_2))\n",
        "print('-----')\n",
        "\n",
        "response = myChain_2.invoke(input={\n",
        "    \"output_language\": \"Kannada\",\n",
        "    \"lines\": 5,\n",
        "    \"text\": \"I love artificail intelligence\"\n",
        "})\n",
        "\n",
        "print('-----')\n",
        "print(response)\n",
        "print(type(response))\n",
        "print('-----')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-A40qoCskSE",
        "outputId": "7f6fa971-7f09-41ae-ed38-2712bf182feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "input_variables=['lines', 'output_language', 'text'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_language'], input_types={}, partial_variables={}, template='You are a top class poet in {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['lines', 'text'], input_types={}, partial_variables={}, template='Give me a {lines} lines of poem about the {text}'), additional_kwargs={})]\n",
            "-----\n",
            "-----\n",
            "first=ChatPromptTemplate(input_variables=['lines', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_language'], input_types={}, partial_variables={}, template='You are a top class poet in {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['lines', 'text'], input_types={}, partial_variables={}, template='Give me a {lines} lines of poem about the {text}'), additional_kwargs={})]) middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
            "-----\n",
            "-----\n",
            "ಕಲಾತ್ಮಕ ಬುದ್ಧಿಯ ಸೆಳೆತಕ್ಕೆ ನಾನು ಕೈ ಬಿಡೆ\n",
            "ಯಂತ್ರಗಳ ಹೃದಯದಲ್ಲಿ ಕಲೆಯಿಂದ ಸಾಕಾರ\n",
            "ಮಾನವತೆಯ ಕೃತಿ ಇದು, ನಾನು ಇಷ್ಟಪಡುತ್ತೇನೆ\n",
            "ಅವನ ಲೆಕ್ಕಗಳಲ್ಲಿ ಕಲೆಯಿಂದ ಒಂದು ಸಂಸಾರ\n",
            "ಕೃತಕ ಬುದ್ಧಿಯಲ್ಲಿ ನಾನು ಪ್ರೀತಿಗೆ ಸಿಲುಕುತ್ತೇನೆ\n",
            "\n",
            "(I love the allure of artistic intelligence\n",
            "In the heart of machines, a work of art takes shape\n",
            "This is a human creation, I love it\n",
            "In its calculations, a world of art is revealed\n",
            "I'm smitten with love for artificial intelligence)\n",
            "<class 'str'>\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain Method - Parallel"
      ],
      "metadata": {
        "id": "FKuK4DP5uxcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parallel LCEL Chain**\n",
        "- Parallel chains are used to run multiple runnables in parallel.\n",
        "- The final return value is a dict with the results of each value under its appropriate key.\n",
        "\n",
        "\n",
        "**What is a Parallel Chain?**\n",
        "\n",
        "- Parallel chaining lets you run multiple chains (pipelines) simultaneously using the same or similar inputs, collecting all outputs together.\n",
        "\n",
        "- In LangChain LCEL, this is achieved via RunnableParallel.\n",
        "\n",
        "- It's analogous to a “fan-out” pattern, where you branch out your input to multiple tasks and gather all their results in one go.\n",
        "\n",
        "**Why Use Parallel Chains?**\n",
        "\n",
        "- To answer multiple questions about the same input in one shot.\n",
        "- To generate diverse content (e.g., facts, poems, summaries) from the same base information.\n",
        "- To save code and avoid running chains sequentially when there's no dependency between them.\n",
        "\n"
      ],
      "metadata": {
        "id": "A57xqi2qB-1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "myParallelChain = RunnableParallel(\n",
        "    translate=myChain,\n",
        "    poem=myChain_2\n",
        ")\n",
        "\n",
        "print('-----')\n",
        "print(myParallelChain)\n",
        "print(type(myParallelChain))\n",
        "print('-----')\n",
        "\n",
        "response = myParallelChain.invoke(input={\n",
        "    \"input_language\": \"English\",\n",
        "    \"output_language\": \"Kannada\",\n",
        "    \"text\": \"Artificial Intelligence\",\n",
        "    \"lines\": 5,\n",
        "})\n",
        "\n",
        "print('-----')\n",
        "print(response)\n",
        "print(type(response))\n",
        "print('-----')\n",
        "print(response['translate'])\n",
        "print('-----')\n",
        "print(response['poem'])\n",
        "print('-----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs9Zuc98uziV",
        "outputId": "9cabe331-8812-4564-d052-9c6930ff440f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "steps__={'translate': ChatPromptTemplate(input_variables=['input_language', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])\n",
            "| ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})\n",
            "| StrOutputParser(), 'poem': ChatPromptTemplate(input_variables=['lines', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_language'], input_types={}, partial_variables={}, template='You are a top class poet in {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['lines', 'text'], input_types={}, partial_variables={}, template='Give me a {lines} lines of poem about the {text}'), additional_kwargs={})])\n",
            "| ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})\n",
            "| StrOutputParser()}\n",
            "<class 'langchain_core.runnables.base.RunnableParallel'>\n",
            "-----\n",
            "-----\n",
            "{'translate': 'ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)', 'poem': 'Here is a 5-line poem in Kannada about Artificial Intelligence:\\n\\n\\nಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\\nಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\\nಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\\nಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಡುಕುತಾನೆ\\nಸೃಷ್ಟಿಯ ರಹಸ್ಯವನ್ನು ಅರಿಯುತಾನೆ'}\n",
            "<class 'dict'>\n",
            "-----\n",
            "ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)\n",
            "-----\n",
            "Here is a 5-line poem in Kannada about Artificial Intelligence:\n",
            "\n",
            "\n",
            "ಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\n",
            "ಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\n",
            "ಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\n",
            "ಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಡುಕುತಾನೆ\n",
            "ಸೃಷ್ಟಿಯ ರಹಸ್ಯವನ್ನು ಅರಿಯುತಾನೆ\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain Method - Composed Sequencial\n",
        "\n",
        "Passing data to next chain / runnable - using input parameter for next chain's prompt."
      ],
      "metadata": {
        "id": "HFFbmO1fwtqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Straight forward method**"
      ],
      "metadata": {
        "id": "9I4YcVfrzOhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "analysis_template = PromptTemplate.from_template('''\n",
        "Analyse the following: {response}.\n",
        "You need to tell me which language it is written in.\n",
        "You just need to answer in one sentence.\n",
        "''')\n",
        "\n",
        "response_of_chain_2 = myChain_2.invoke({\n",
        "    \"output_language\": \"Kannada\",\n",
        "    \"lines\": 5,\n",
        "    \"text\": \"Artificial Intelligence\"\n",
        "})\n",
        "\n",
        "analysis_chain = analysis_template | llm | StrOutputParser()\n",
        "\n",
        "response_analysis_chain = analysis_chain.invoke({\n",
        "    \"response\": response_of_chain_2\n",
        "})\n",
        "\n",
        "print('-----')\n",
        "print(response_analysis_chain)\n",
        "print('-----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTlbXbwiw5JD",
        "outputId": "c97e07ab-ace7-42f6-c23e-bca38eeef27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "The poem is written in Kannada, a Dravidian language spoken in the Indian state of Karnataka.\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using composed chains**"
      ],
      "metadata": {
        "id": "VwpWSxjbzZR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "analysis_template = PromptTemplate.from_template('''\n",
        "Analyse the following: {response}.\n",
        "You need to tell me which language it is written in.\n",
        "You just need to answer in one sentence.\n",
        "''')\n",
        "\n",
        "analysis_chain = analysis_template | llm | StrOutputParser()\n",
        "\n",
        "# 2nd Runnable is expecting \"response\" in it's input variable\n",
        "\n",
        "composed_chain = {\"response\" : myChain_2} | analysis_chain\n",
        "\n",
        "response_composed_chain = composed_chain.invoke({\n",
        "    \"output_language\": \"Kannada\",\n",
        "    \"lines\": 5,\n",
        "    \"text\": \"Artificial Intelligence\"\n",
        "})\n",
        "\n",
        "print('-----')\n",
        "print(response_composed_chain)\n",
        "print('-----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF5X9WrDzg6l",
        "outputId": "efbbd792-52c6-4faa-f35b-fae923be8207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "The poem is written in Kannada, a Dravidian language spoken in the Indian state of Karnataka.\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain Router\n"
      ],
      "metadata": {
        "id": "U_RfghaQDZ-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The router chain is used to route the output of a previous runnable to the next runnable based on the output of the previous runnable."
      ],
      "metadata": {
        "id": "2ch6bitiDlrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment analyzer**"
      ],
      "metadata": {
        "id": "tsJR7M8XG50-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = \"\"\"\n",
        "Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
        "Do not respond with more than one word.\n",
        "\n",
        "Review: {review}\n",
        "Classification:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"review\"],\n",
        "    template=prompt,\n",
        ")\n",
        "\n",
        "print('-----')\n",
        "print(prompt_template)\n",
        "print('-----')\n",
        "\n",
        "sentiment_chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "print('-----')\n",
        "print(sentiment_chain)\n",
        "print('-----')\n",
        "\n",
        "review = \"The battery life is exceptional. I charged this device four weeks ago and have read two full novels and multiple articles without the battery indicator dropping below 60%. The screen also causes zero eye strain, even after hours of continuous reading. This device is the definitive upgrade.\"\n",
        "# review = \"The experience was severely disappointing. The wait time for the main courses was over an hour, and when the food finally arrived, the seafood platter was cold and clearly overcooked, tasting rubbery. The waiter was inattentive and provided no updates or apologies for the significant delay. The high prices are not justified by the subpar quality of the food and service. Avoid this place.\"\n",
        "\n",
        "sentiment_chain.invoke({\"review\": review})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "HFjx6offDtJM",
        "outputId": "e7e1bc96-979e-412d-a71c-2cc257ffdbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "input_variables=['review'] input_types={} partial_variables={} template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n'\n",
            "-----\n",
            "-----\n",
            "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
            "-----\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive mail composer**"
      ],
      "metadata": {
        "id": "McFKKoMqG-WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_mail_prompt = \"\"\"\n",
        "You are expert in writing reply for positive reviews.\n",
        "You need to encourage the user to share their experience on social media.\n",
        "Review: {review}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "positive_mail_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"review\"],\n",
        "    template=positive_mail_prompt,\n",
        ")\n",
        "\n",
        "print('-----')\n",
        "print(positive_mail_prompt_template)\n",
        "print('-----')\n",
        "\n",
        "positive_mail_chain = positive_mail_prompt_template | llm | StrOutputParser()\n",
        "\n",
        "print('-----')\n",
        "print(positive_mail_chain)\n",
        "print('-----')\n",
        "\n",
        "review = \"The battery life is exceptional. I charged this device four weeks ago and have read two full novels and multiple articles without the battery indicator dropping below 60%. The screen also causes zero eye strain, even after hours of continuous reading. This device is the definitive upgrade.\"\n",
        "\n",
        "response = positive_mail_chain.invoke({\"review\": review})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXFGVLPFHHYh",
        "outputId": "a19bc2c6-27fa-4201-bdbc-f7bd9adda8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "input_variables=['review'] input_types={} partial_variables={} template='\\nYou are expert in writing reply for positive reviews.\\nYou need to encourage the user to share their experience on social media.\\nReview: {review}\\nAnswer:\\n'\n",
            "-----\n",
            "-----\n",
            "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nYou are expert in writing reply for positive reviews.\\nYou need to encourage the user to share their experience on social media.\\nReview: {review}\\nAnswer:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
            "-----\n",
            "Wow, thank you so much for sharing your amazing experience with our device! We're thrilled to hear that the battery life has exceeded your expectations, and that you've been able to enjoy your favorite novels and articles without any interruptions. It's also great to know that our screen technology has been easy on your eyes, even during extended reading sessions.\n",
            "\n",
            "We'd love for you to share your experience with others! Would you consider posting about your experience on social media? A quick share on Facebook, Twitter, or Instagram can help others discover the benefits of our device. Your endorsement can make a huge difference in helping others find their perfect reading companion. Thank you again for your wonderful review, and we're so glad you're enjoying your device!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Negative mail composer**"
      ],
      "metadata": {
        "id": "60McIugEHESj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_mail_prompt = \"\"\"\n",
        "You are expert in writing reply for negative reviews.\n",
        "You need first to apologize for the inconvenience caused to the user.\n",
        "You need to encourage the user to share their concern on following Email:'sam@mobi.in'.\n",
        "Review: {review}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "negative_mail_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"review\"],\n",
        "    template=negative_mail_prompt,\n",
        ")\n",
        "\n",
        "print('-----')\n",
        "print(negative_mail_prompt_template)\n",
        "print('-----')\n",
        "\n",
        "negative_mail_chain = negative_mail_prompt_template | llm | StrOutputParser()\n",
        "\n",
        "print('-----')\n",
        "print(negative_mail_chain)\n",
        "print('-----')\n",
        "\n",
        "review = \"The experience was severely disappointing. The wait time for the main courses was over an hour, and when the food finally arrived, the seafood platter was cold and clearly overcooked, tasting rubbery. The waiter was inattentive and provided no updates or apologies for the significant delay. The high prices are not justified by the subpar quality of the food and service. Avoid this place.\"\n",
        "\n",
        "response = negative_mail_chain.invoke({\"review\": review})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFZT88jbIEl1",
        "outputId": "0d9b1018-874f-4d14-baee-3c9bb27abcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "input_variables=['review'] input_types={} partial_variables={} template=\"\\nYou are expert in writing reply for negative reviews.\\nYou need first to apologize for the inconvenience caused to the user.\\nYou need to encourage the user to share their concern on following Email:'sam@mobi.in'.\\nReview: {review}\\nAnswer:\\n\"\n",
            "-----\n",
            "-----\n",
            "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template=\"\\nYou are expert in writing reply for negative reviews.\\nYou need first to apologize for the inconvenience caused to the user.\\nYou need to encourage the user to share their concern on following Email:'sam@mobi.in'.\\nReview: {review}\\nAnswer:\\n\") middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
            "-----\n",
            "Here's a potential response to the negative review:\n",
            "\n",
            "\"Dear valued customer,\n",
            "\n",
            "We are truly sorry to hear that your experience at our restaurant was severely disappointing. We apologize for the excessive wait time for your main courses and the unacceptable quality of the seafood platter. It's clear that we fell short of our usual standards, and for that, we are truly sorry.\n",
            "\n",
            "We understand that our high prices are only justified by exceptional food and service, and it's clear that we failed to deliver on both counts during your visit. We take full responsibility for the inattentive service and lack of updates from our waiter.\n",
            "\n",
            "We would like to make things right and invite you to share your concerns with us in more detail. Please email us at sam@mobi.in so we can better understand what went wrong and take corrective action to prevent such incidents in the future. Your feedback is invaluable to us, and we appreciate your time in sharing your experience.\n",
            "\n",
            "Once again, we apologize for the inconvenience and disappointment caused, and we hope to have the opportunity to serve you better in the future.\n",
            "\n",
            "Sincerely, [Your Name]\"\n",
            "\n",
            "This response acknowledges the customer's disappointment, apologizes for the specific issues mentioned, and encourages them to share their concerns with the restaurant. By providing a specific email address, the customer has a clear channel to communicate their feedback and help the restaurant improve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision function**"
      ],
      "metadata": {
        "id": "0ZZ3PXcQJvro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decisionReviewer(info):\n",
        "    if 'positive' in info['sentiment'].lower():\n",
        "        return positive_mail_chain\n",
        "    else:\n",
        "        return negative_mail_chain"
      ],
      "metadata": {
        "id": "aZWer-ZhJ0HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "print('-----')\n",
        "print(sentiment_chain)\n",
        "print('-----')\n",
        "\n",
        "# the input of full_chain Runnable Sequence is passed as x for the lambda function\n",
        "\n",
        "full_chain = {'sentiment': sentiment_chain, 'review': lambda x: x['review']} | RunnableLambda(decisionReviewer)\n",
        "\n",
        "print('-----')\n",
        "print(full_chain)\n",
        "print(type(full_chain))\n",
        "print('-----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwCZvTQ6KD9S",
        "outputId": "864a1919-caa7-4e69-ba42-1b676d4fd3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "first=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
            "-----\n",
            "-----\n",
            "first={\n",
            "  sentiment: PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='\\nGiven the user review below, classify it as either being about `Positive` or `Negative`.\\nDo not respond with more than one word.\\n\\nReview: {review}\\nClassification:\\n')\n",
            "             | ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})\n",
            "             | StrOutputParser(),\n",
            "  review: RunnableLambda(lambda x: x['review'])\n",
            "} middle=[] last=RunnableLambda(decisionReviewer)\n",
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive Test\n",
        "\n",
        "review = \"The battery life is exceptional. I charged this device four weeks ago and have read two full novels and multiple articles without the battery indicator dropping below 60%. The screen also causes zero eye strain, even after hours of continuous reading. This device is the definitive upgrade.\"\n",
        "\n",
        "response = full_chain.invoke({\"review\": review})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz5DoBr2M9Yg",
        "outputId": "242c3dc8-2056-43fb-bf57-d4b8077f6da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow, thank you so much for sharing your amazing experience with our device! We're thrilled to hear that the battery life has exceeded your expectations, and that you've been able to enjoy your favorite novels and articles without any interruptions. It's also great to know that our screen technology has been easy on your eyes, even during extended reading sessions.\n",
            "\n",
            "We'd love for you to share your experience with others! Would you consider posting about your experience on social media? A quick share on Facebook, Twitter, or Instagram can help others discover the benefits of our device. Your endorsement can make a huge difference in helping others find their perfect reading companion. Thank you again for your wonderful review, and we're so glad you're enjoying your device!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative Test\n",
        "\n",
        "review = \"The experience was severely disappointing. The wait time for the main courses was over an hour, and when the food finally arrived, the seafood platter was cold and clearly overcooked, tasting rubbery. The waiter was inattentive and provided no updates or apologies for the significant delay. The high prices are not justified by the subpar quality of the food and service. Avoid this place.\"\n",
        "\n",
        "response = full_chain.invoke({\"review\": review})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kcr7fHp0M_8C",
        "outputId": "7a33865a-a6e0-42e4-e56e-07ac8e77d0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a potential response to the negative review:\n",
            "\n",
            "\"Dear valued customer,\n",
            "\n",
            "We are truly sorry to hear that your experience at our restaurant was severely disappointing. We apologize for the excessive wait time for your main courses and the unacceptable quality of the seafood platter. It's clear that we fell short of our usual standards, and for that, we are truly sorry.\n",
            "\n",
            "We understand that our high prices are only justified by exceptional food and service, and it's clear that we failed to deliver on both counts during your visit. We take full responsibility for the inattentive service and lack of updates from our waiter.\n",
            "\n",
            "We would like to make things right and invite you to share your concerns with us in more detail. Please email us at sam@mobi.in so we can better understand what went wrong and take corrective action to prevent such incidents in the future. Your feedback is invaluable to us, and we appreciate your time in sharing your experience.\n",
            "\n",
            "Once again, we apologize for the inconvenience and disappointment caused, and we hope to have the opportunity to serve you better in the future.\n",
            "\n",
            "Sincerely, [Your Name]\"\n",
            "\n",
            "This response acknowledges the customer's disappointment, apologizes for the specific issues mentioned, and encourages them to share their concerns in more detail via email. By showing empathy and a willingness to listen and improve, we can potentially turn a negative experience into a positive one and show that we value our customers' feedback.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Chain Runnables"
      ],
      "metadata": {
        "id": "eEjuF3oqQMmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is useful for formatting or when you need functionality not provided by other LangChain components.\n",
        "- Custom functions used as Runnables are called `RunnableLambdas`."
      ],
      "metadata": {
        "id": "lQeLssYQQYxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "def char_counts(text):\n",
        "    return len(text)\n",
        "\n",
        "def word_counts(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def pricing(text):\n",
        "    return len(text.split()) * 0.001\n",
        "\n",
        "prompt = \"\"\"\n",
        "Explain below topic in {lines} bullet point lines.\n",
        "\n",
        "Topic: {topic}\n",
        "Explaination:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"lines\"],\n",
        "    template=prompt,\n",
        ")\n",
        "\n",
        "print('-----')\n",
        "print(prompt_template)\n",
        "print('-----')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INTR10JSQgnw",
        "outputId": "d56746f5-1b0f-4748-f7cd-f056e2cc1d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "input_variables=['lines', 'topic'] input_types={} partial_variables={} template='\\nExplain below topic in {lines} bullet point lines.\\n\\nTopic: {topic}\\nExplaination:\\n'\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ususal run test\n",
        "\n",
        "explainer_chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "print('-----')\n",
        "print(explainer_chain)\n",
        "print('-----')\n",
        "\n",
        "response = explainer_chain.invoke({\n",
        "    \"topic\": \"Artificial Intelligence\",\n",
        "    \"lines\": 5,\n",
        "})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXb21PskShod",
        "outputId": "cd63388d-46f4-4787-dfa6-326b27be9041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "first=PromptTemplate(input_variables=['lines', 'topic'], input_types={}, partial_variables={}, template='\\nExplain below topic in {lines} bullet point lines.\\n\\nTopic: {topic}\\nExplaination:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={})] last=StrOutputParser()\n",
            "-----\n",
            "Here are 5 bullet point lines explaining Artificial Intelligence (AI):\n",
            "\n",
            "• **Definition**: Artificial Intelligence is a technology that creates intelligent machines that can think, learn, and act like humans, enabling them to perform tasks that typically require human intelligence.\n",
            "\n",
            "• **Types**: There are several types of AI, including Narrow or Weak AI, which is designed to perform a specific task, and General or Strong AI, which is designed to perform any intellectual task that a human can.\n",
            "\n",
            "• **How it Works**: AI systems use algorithms and data to learn, make decisions, and improve their performance over time. They can also use machine learning and deep learning techniques to analyze data and make predictions.\n",
            "\n",
            "• **Applications**: AI has a wide range of applications, including virtual assistants, image and speech recognition, natural language processing, predictive maintenance, and self-driving cars.\n",
            "\n",
            "• **Benefits**: The benefits of AI include increased efficiency, improved accuracy, and enhanced decision-making capabilities, as well as the potential to revolutionize industries such as healthcare, finance, and education.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with runnables\n",
        "\n",
        "explainer_chain = prompt_template | llm | StrOutputParser() | {\n",
        "    \"char_count\": RunnableLambda(char_counts),\n",
        "    \"word_count\": RunnableLambda(word_counts),\n",
        "    \"cost\": RunnableLambda(pricing),\n",
        "    'output': RunnablePassthrough(), #RunnableLambda(passthrough)\n",
        "}\n",
        "\n",
        "print('-----')\n",
        "print(explainer_chain)\n",
        "print('-----')\n",
        "\n",
        "response = explainer_chain.invoke({\n",
        "    \"topic\": \"Artificial Intelligence\",\n",
        "    \"lines\": 5,\n",
        "})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR2yvua2Sn_V",
        "outputId": "81fb5e55-8115-4a5f-f96a-931cc621a5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "first=PromptTemplate(input_variables=['lines', 'topic'], input_types={}, partial_variables={}, template='\\nExplain below topic in {lines} bullet point lines.\\n\\nTopic: {topic}\\nExplaination:\\n') middle=[ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='meta/llama-3.1-405b-instruct', default_headers={}), StrOutputParser()] last={\n",
            "  char_count: RunnableLambda(char_counts),\n",
            "  word_count: RunnableLambda(word_counts),\n",
            "  cost: RunnableLambda(pricing),\n",
            "  output: RunnablePassthrough()\n",
            "}\n",
            "-----\n",
            "{'char_count': 1134, 'word_count': 162, 'cost': 0.162, 'output': 'Here are 5 bullet point lines explaining Artificial Intelligence (AI):\\n\\n• **Definition**: Artificial Intelligence is a technology that creates intelligent machines that can think, learn, and act like humans, enabling them to perform tasks that typically require human intelligence.\\n\\n• **Types**: There are several types of AI, including Narrow or Weak AI, which is designed to perform a specific task, and General or Strong AI, which can perform any intellectual task that a human can.\\n\\n• **How it Works**: AI uses algorithms and data to learn, reason, and self-improve, allowing it to make decisions and take actions autonomously, without being explicitly programmed.\\n\\n• **Applications**: AI has numerous applications, including virtual assistants, image recognition, natural language processing, robotics, and predictive analytics, which are used in industries such as healthcare, finance, and transportation.\\n\\n• **Benefits**: The benefits of AI include increased efficiency, productivity, and accuracy, as well as the potential to solve complex problems and make new discoveries, leading to improved decision-making and innovation.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Chain using `@chain` decorator\n",
        "\n"
      ],
      "metadata": {
        "id": "VUt3fiSvUmpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. What is the `@chain` Decorator?\n",
        "\n",
        "- The @chain decorator is a feature of LangChain's LCEL API (Expression Language).\n",
        "- It transforms any standard Python function into a Runnable Chain.\n",
        "- This allows you to wrap arbitrary Python logic—including loops, conditionals, and composition of other chains—into a component that behaves just like any other chain in LCEL (i.e., it supports .invoke(), .batch(), etc.).\n",
        "\n",
        "\n",
        "2. Why Use `@chain`?\n",
        "\n",
        "- For complex, custom workflows that can't be easily expressed by chaining with the | operator.\n",
        "- To combine multiple sub-chains, add custom logic, or perform steps that need Python code (like aggregation, formatting, conditional logic).\n",
        "- It enables full flexibility within the LCEL framework: your custom function becomes a first-class Runnable.\n",
        "\n",
        "\n",
        "4. How Does it Work?\n",
        "\n",
        "- The decorator wraps your function into a Runnable-compatible object.\n",
        "- Supports all LCEL operations (invoke, batch, etc.).\n",
        "- You can include arbitrary Python code—loops, conditionals, error handling, etc.\n",
        "- The function input is a single parameter (often a dict).\n",
        "- The output can be any object—dict, list, string, etc.\n"
      ],
      "metadata": {
        "id": "gKcSqCzaUs3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "@chain\n",
        "def custom_chain(params):\n",
        "    return {\n",
        "        'translation': myChain.invoke(params),\n",
        "        'poem': myChain_2.invoke(params),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "cjMmNRibVh_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"input_language\": \"English\",\n",
        "    \"output_language\": \"Kannada\",\n",
        "    \"text\": \"Artificial Intelligence\",\n",
        "    \"lines\": 5,\n",
        "}\n",
        "\n",
        "response = custom_chain.invoke(params)\n",
        "\n",
        "print('-----')\n",
        "print(response)\n",
        "print('-----')\n",
        "print(response['translation'])\n",
        "print('-----')\n",
        "print(response['poem'])\n",
        "print('-----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggd_zuSFWgND",
        "outputId": "13ce68fb-8e4c-4b92-a379-23a4e4d64396",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "{'translation': 'ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)', 'poem': 'Here is a 5-line poem in Kannada about Artificial Intelligence:\\n\\n\\nಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\\nಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\\nಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\\nಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಟ್ಟಿದೆ\\nನಮ್ಮ ಭಾವಿಯ ನೀಲನಕ್ಷೆ\\n\\n\\n(Translation: \\n\\nThe power of Artificial Intelligence\\nA challenge to human imagination\\nIn the heart of the computer\\nA spark of intelligence is born\\nA blueprint for our future)'}\n",
            "-----\n",
            "ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ (Kruthaka buddhimatte)\n",
            "-----\n",
            "Here is a 5-line poem in Kannada about Artificial Intelligence:\n",
            "\n",
            "\n",
            "ಕೃತ್ರಿಮ ಬುದ್ಧಿಯ ಸಾಮರ್ಥ್ಯ\n",
            "ಮಾನವನ ಕಲ್ಪನೆಗೆ ಸವಾಲು\n",
            "ಗಣಕ ಯಂತ್ರದ ಹೃದಯದಲ್ಲಿ\n",
            "ಬುದ್ಧಿಶಕ್ತಿಯ ಮಚ್ಚು ಹುಟ್ಟಿದೆ\n",
            "ನಮ್ಮ ಭಾವಿಯ ನೀಲನಕ್ಷೆ\n",
            "\n",
            "\n",
            "(Translation: \n",
            "\n",
            "The power of Artificial Intelligence\n",
            "A challenge to human imagination\n",
            "In the heart of the computer\n",
            "A spark of intelligence is born\n",
            "A blueprint for our future)\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3"
      ],
      "metadata": {
        "id": "X_XriBSxzupY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contents\n",
        "\n",
        "* Pydantic\n",
        "* String Output Parser (StrOutputParser)\n",
        "* JsonOutputParser"
      ],
      "metadata": {
        "id": "gUHudWPC0Glh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pydantic in LangChain"
      ],
      "metadata": {
        "id": "aQe1UpoFhlgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Pydantic?**\n",
        "\n",
        "- **Pydantic** is a Python library for **data validation** and **settings management** using Python type annotations.\n",
        "- It provides a `BaseModel` class for declaring data models with types, constraints, and descriptions.\n",
        "- It **automatically validates** and parses data, raising errors for missing or invalid fields.\n",
        "\n",
        "**In LangChain, Pydantic models are often used for:**\n",
        "- Defining structured outputs expected from an LLM (e.g., a Joke object, a product recommendation, etc.).\n",
        "- Validating and parsing LLM outputs into safe, strongly-typed Python objects."
      ],
      "metadata": {
        "id": "DZq0duzShzp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import  Optional\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "7kqSUI1wh-CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analogy ---- Create a table named Joke which has 3 columns\n",
        "#             setup : str , not null\n",
        "#             punchline: str , not null\n",
        "#             rating: int , null, None, value between 1 to 10\n",
        "\n",
        "# Constraints for Field object\n",
        "#==============================\n",
        "# ge --- greater than equal to\n",
        "# le --- less than equal to\n",
        "# gt --- greater than\n",
        "# lt --- less than\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline of the joke\")\n",
        "    rating: Optional[int] = Field(description=\"The rating of the joke is from 1 to 10\", default=None, ge=1, le=10)\n"
      ],
      "metadata": {
        "id": "p0aXAQNUjs3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parsing"
      ],
      "metadata": {
        "id": "mpfMODDoaqbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language models outputs the text. But there are times where you want to get more structured information than just text back.\n",
        "\n",
        "Output parsers are classes that help structure language model responses.\n",
        "\n",
        "There are two main methods an output parser must implement:\n",
        "\n",
        "- **Get format instructions**: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
        "- **Parse**: A method which takes in a string (assumed to be the response from a LLM) and parses it into some structure.\n",
        "\n",
        "\n",
        "- Output Parsing\n",
        "    - StrOutputParser\n",
        "    - JsonOutputParser\n",
        "    - CSV Output Parser\n",
        "    - Datetime Output Parser\n",
        "    - Structured Output Parser (Pydanitc or Json)"
      ],
      "metadata": {
        "id": "oyG-LktLaucp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StrOutputParser"
      ],
      "metadata": {
        "id": "RvusZ1BZc_iP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `StrOutputParser` is a utility class in LangChain for handling the output of language models (LLMs/ChatModels).\n",
        "\n",
        "- It parses the model's response and returns it as a plain Python string, removing any unnecessary metadata, wrappers, or objects that the model or API may return.\n",
        "\n",
        "\n",
        "**Why do we need it?**\n",
        "\n",
        "- By default, LLMs or chat models in LangChain might return structured objects, message objects, or dictionaries.\n",
        "- In most cases, especially for simple chains, you just want the generated text string (e.g., the answer, summary, completion, etc.).\n",
        "- `StrOutputParser` extracts this text from the raw output so your chain returns a clean string.\n"
      ],
      "metadata": {
        "id": "dT_vMjtXdIcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Tell me a joke about {topic}\"\n",
        ")\n",
        "\n",
        "my_chain = prompt | llm | parser\n",
        "\n",
        "response_clean = my_chain.invoke({'topic': 'Artificial Intelligence'})\n",
        "print(response_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzYq_D7VauGc",
        "outputId": "f299790f-3653-4eca-c3cb-b1f308e829ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's one:\n",
            "\n",
            "Why did the AI program go to therapy?\n",
            "\n",
            "Because it was struggling to process its emotions! (get it?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JsonOutputParser"
      ],
      "metadata": {
        "id": "ptYqb2FcjN40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`JsonOutputParser` converts LLM text responses into JSON objects that your code can work with directly.\n",
        "\n",
        "The parser takes the model's raw text output and transforms it into a structured Python dictionary or Pydantic model. This is useful when you want the LLM to return data in a specific format rather than plain text."
      ],
      "metadata": {
        "id": "QxolKLQujYSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you invoke the chain, the `JsonOutputParser`:\n",
        "\n",
        "* Calls the LLM with the prompt\n",
        "* The model generates JSON text based on `get_format_instructions()`\n",
        "* The parser converts the JSON text into your defined structure (Pydantic model or dict)\n",
        "* Returns the parsed object instead of raw text"
      ],
      "metadata": {
        "id": "4Otr7pA4jx7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "L7F5ct-mjWf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the expected output structure\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"The person's name\")\n",
        "    age: int = Field(description=\"The person's age\")\n",
        "    city: str = Field(description=\"The city they live in\")\n",
        "\n",
        "# Create a parser\n",
        "parser = JsonOutputParser(pydantic_object=Person)"
      ],
      "metadata": {
        "id": "83KzQmn8kpaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build your prompt with format instructions\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Extract person information.\"),\n",
        "    (\"user\", \"Extract information from: {input}\\n{format_instructions}\"),\n",
        "]).partial(format_instructions=parser.get_format_instructions())\n",
        "\n",
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "80nwMxJMkvjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "result = chain.invoke({\"input\": \"My self Sam, im 28 years old and I live in New York\"})\n",
        "\n",
        "print('--------')\n",
        "print(result)\n",
        "print('--------')\n",
        "print(type(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmqPtMQ9k7lp",
        "outputId": "ebc6066d-7811-4e39-f7bf-97a668892fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "{'name': 'Sam', 'age': 28, 'city': 'New York'}\n",
            "--------\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    }
  ]
}